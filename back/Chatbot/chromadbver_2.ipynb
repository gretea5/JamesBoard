{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ea1908-7d46-4d3b-aef7-d4d9aa3dd27c",
   "metadata": {},
   "source": [
    "# ë³´ë“œê²Œì„ ì¶”ì²œ ì‹œìŠ¤í…œ í†µí•© íŒŒì´í”„ ë¼ì¸\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë³´ë“œê²Œì„ ë¦¬ë·°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ë²¡í„°í™”í•˜ê³  ChromaDBì— ì €ì¥í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ ì¿¼ë¦¬ë¡œ ì˜ì–´ ë¦¬ë·°ì™€ ê²Œì„ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í¬ë¡œìŠ¤ ì–¸ì–´ RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ae851-40be-46c8-a692-08cf2ff51f5c",
   "metadata": {},
   "source": [
    "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62188122-7380-4ea0-8dd4-d5be0ce1d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython==8.16.1\n",
    "# ë…¸íŠ¸ë¶ ì…€ì—ì„œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -U langchain-huggingface langchain-chroma langchain-core langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351cc344-21fb-42d4-89b8-72e96bc727fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)\n",
    "!pip install sentence-transformers langchain langchain-community chromadb pandas tqdm torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cc412-ab48-4e28-bea2-b0510d2e82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6ac20-febb-4dcb-a057-b1771f89327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!jupyter nbextension install --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382852e-2257-4f39-b760-756475a48341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69730a7-334b-45b3-85af-925ddddd97b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896deec-62db-4c64-9a26-6cdc047ae566",
   "metadata": {},
   "source": [
    "# ë²„ê·¸ ê°œì„ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3575a16-2092-480a-a188-f4ff754585fd",
   "metadata": {},
   "source": [
    "### 1) gpu ë¯¸ì‚¬ìš© ê°œì„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0ac13-1ee3-4527-b4d1-2cdec3c4cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1. ê¸°ì¡´ íŒ¨í‚¤ì§€ ì œê±° ì‹œì‘\")\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "print(\"ê¸°ì¡´ íŒ¨í‚¤ì§€ ì œê±° ì™„ë£Œ\")\n",
    "print(\"pip ìºì‹œ ì´ˆê¸°í™”\")\n",
    "!pip cache purge\n",
    "print(\"2. CUDA ë²„ì „ ì„¤ì¹˜ ì‹œì‘ (ìƒì„¸ ë¡œê·¸ í™œì„±í™”)\")\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --verbose\n",
    "print(\"ì„¤ì¹˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ae170-5413-48b4-ad3e-7f9dc8ce6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"í™•ì¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f619fc8-0160-4721-be63-af4438605443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1be2cd-314d-4b12-84e7-00a82121a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_status_safe():\n",
    "    \"\"\"GPU ìƒíƒœë¥¼ ì•ˆì „í•˜ê²Œ ì§„ë‹¨í•©ë‹ˆë‹¤(íƒ€ì„ì•„ì›ƒ ë° ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)\"\"\"\n",
    "    print(\"=== GPU ì§„ë‹¨ ì‹œì‘ ===\")\n",
    "    \n",
    "    import torch\n",
    "    import platform\n",
    "    import subprocess\n",
    "    import threading\n",
    "    import time\n",
    "    \n",
    "    # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Python: {platform.python_version()}\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    \n",
    "    # CUDA ê°€ìš©ì„± í™•ì¸\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {cuda_available}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        try:\n",
    "            print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "            print(f\"GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"CUDA ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    else:\n",
    "        print(\"CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°€ëŠ¥í•œ ì›ì¸:\")\n",
    "        \n",
    "        # íƒ€ì„ì•„ì›ƒ ê¸°ëŠ¥ì´ ìˆëŠ” subprocess ì‹¤í–‰ í•¨ìˆ˜\n",
    "        def run_with_timeout(cmd, timeout=10):\n",
    "            result = {\"completed\": False, \"output\": \"\"}\n",
    "            \n",
    "            def target():\n",
    "                try:\n",
    "                    process = subprocess.Popen(\n",
    "                        cmd, \n",
    "                        shell=True, \n",
    "                        stdout=subprocess.PIPE, \n",
    "                        stderr=subprocess.PIPE,\n",
    "                        text=True\n",
    "                    )\n",
    "                    stdout, stderr = process.communicate(timeout=timeout)\n",
    "                    result[\"output\"] = stdout\n",
    "                    result[\"completed\"] = True\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    result[\"output\"] = f\"ëª…ë ¹ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ ({timeout}ì´ˆ)\"\n",
    "                except Exception as e:\n",
    "                    result[\"output\"] = f\"ì˜¤ë¥˜: {e}\"\n",
    "            \n",
    "            thread = threading.Thread(target=target)\n",
    "            thread.start()\n",
    "            thread.join(timeout=timeout+1)\n",
    "            \n",
    "            if thread.is_alive():\n",
    "                print(f\"ê²½ê³ : ëª…ë ¹ ì‹¤í–‰ì´ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
    "                return f\"ì‹œê°„ ì´ˆê³¼ ({timeout}ì´ˆ)\"\n",
    "            \n",
    "            return result[\"output\"] if result[\"completed\"] else result[\"output\"]\n",
    "        \n",
    "        # NVIDIA GPU ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì•ˆì „í•œ ì‹¤í–‰)\n",
    "        if platform.system() == 'Windows':\n",
    "            print(\"Windowsì—ì„œ GPU ì •ë³´ í™•ì¸ ì¤‘... (ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°)\")\n",
    "            gpu_info = run_with_timeout('nvidia-smi')\n",
    "            if \"NVIDIA\" in gpu_info:\n",
    "                print(\"NVIDIA GPU ë°œê²¬:\")\n",
    "                print(gpu_info[:500]) # ì¶œë ¥ ì œí•œ\n",
    "                print(\"â†’ PyTorchê°€ CUDAì™€ í•¨ê»˜ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "                print(\"â†’ pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "            else:\n",
    "                print(\"â†’ NVIDIA GPUê°€ ê°ì§€ë˜ì§€ ì•Šê±°ë‚˜ ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        elif platform.system() == 'Linux':\n",
    "            print(\"Linuxì—ì„œ GPU ì •ë³´ í™•ì¸ ì¤‘... (ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°)\")\n",
    "            gpu_info = run_with_timeout('nvidia-smi')\n",
    "            if \"NVIDIA\" in gpu_info:\n",
    "                print(\"NVIDIA GPU ë°œê²¬:\")\n",
    "                print(gpu_info[:500]) # ì¶œë ¥ ì œí•œ\n",
    "                print(\"â†’ PyTorchê°€ CUDAì™€ í•¨ê»˜ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "            else:\n",
    "                print(\"â†’ NVIDIA GPUê°€ ê°ì§€ë˜ì§€ ì•Šê±°ë‚˜ ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "                \n",
    "    # PyTorch ë¹Œë“œ ì •ë³´ í™•ì¸ (ì•ˆì „í•˜ê²Œ)\n",
    "    try:\n",
    "        if hasattr(torch, '__config__'):\n",
    "            if hasattr(torch.__config__, 'show'):\n",
    "                print(\"\\nPyTorch ë¹Œë“œ ì •ë³´:\")\n",
    "                build_info = torch.__config__.show()\n",
    "                print(build_info)\n",
    "    except Exception as e:\n",
    "        print(f\"PyTorch ë¹Œë“œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    print(\"=== GPU ì§„ë‹¨ ì™„ë£Œ ===\")\n",
    "    \n",
    "    # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True ë°˜í™˜\n",
    "    return cuda_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02399bc5-b6f1-498e-a93c-542c2ca280c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "# tqdm ì¡°ê±´ë¶€ ì„í¬íŠ¸ (ipywidgets ë¬¸ì œ ë°©ì§€)\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    print(\"âœ… tqdm.notebook ì„±ê³µì ìœ¼ë¡œ ì„í¬íŠ¸ë¨\")\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "    print(\"âš ï¸ tqdm.notebook ì„í¬íŠ¸ ì‹¤íŒ¨ - ì¼ë°˜ tqdm ì‚¬ìš©\")\n",
    "\n",
    "# GPU ì§„ë‹¨ ì‹¤í–‰\n",
    "print(\"\\nğŸ–¥ï¸ GPU ì§„ë‹¨ ì‹œì‘...\")\n",
    "gpu_available = check_gpu_status_safe()\n",
    "print(f\"ğŸ–¥ï¸ GPU ì‚¬ìš© ê°€ëŠ¥: {'âœ… ì˜ˆ' if gpu_available else 'âŒ ì•„ë‹ˆì˜¤'}\")\n",
    "\n",
    "# ë‚˜ë¨¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¡°ê±´ë¶€ ì„í¬íŠ¸\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_core.documents import Document\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    print(\"âœ… LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì„±ê³µ\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ ìµœì‹  LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨\")\n",
    "    try:\n",
    "        from langchain.embeddings import HuggingFaceEmbeddings\n",
    "        from langchain.vectorstores import Chroma\n",
    "        from langchain.schema import Document\n",
    "        from langchain.text_splitters import RecursiveCharacterTextSplitter\n",
    "        print(\"âœ… ë ˆê±°ì‹œ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì„±ê³µ\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        print(\"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install langchain langchain-chroma langchain-core langchain-huggingface langchain-text-splitters\")\n",
    "\n",
    "print(\"\\nğŸ” ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02238d4-ece2-4236-a62d-560664a42d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ì•ˆë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc697c6-2031-4a28-80f8-35e3c239c325",
   "metadata": {},
   "source": [
    "## 2) í”„ë¡œê·¸ë˜ìŠ¤ë°”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1931e13-1db3-41f5-9aa4-2ccceccd88a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ë³„ë„ë¡œ ì‹¤í–‰)\n",
    "# ë¨¼ì € í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ ì¬ì„¤ì¹˜í•©ë‹ˆë‹¤\n",
    "# !pip install --upgrade jupyter notebook ipywidgets\n",
    "\n",
    "# Cell 2: ì„¤ì¹˜ í™•ì¸\n",
    "import ipywidgets\n",
    "print(f\"ipywidgets ë²„ì „: {ipywidgets.__version__}\")\n",
    "\n",
    "# Cell 3: ì§„í–‰ í‘œì‹œì¤„ í…ŒìŠ¤íŠ¸\n",
    "from tqdm.notebook import tqdm\n",
    "for i in tqdm(range(10), desc=\"í…ŒìŠ¤íŠ¸\"):\n",
    "    import time\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd05dab-b0ec-4f26-9aca-6ec4493038bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_progress_bar():\n",
    "    \"\"\"í™˜ê²½ì— ìƒê´€ì—†ì´ ì‘ë™í•˜ëŠ” í”„ë¡œê·¸ë ˆìŠ¤ ë°”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        # ê°€ì¥ ê¸°ë³¸ì ì¸ tqdm ì„í¬íŠ¸ ì‹œë„\n",
    "        from tqdm import tqdm\n",
    "        return tqdm\n",
    "    except ImportError:\n",
    "        # tqdm ìì²´ê°€ ì—†ëŠ” ê²½ìš° ê°€ì§œ í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì‚¬ìš©\n",
    "        class DummyTqdm:\n",
    "            def __init__(self, iterable=None, **kwargs):\n",
    "                self.iterable = iterable\n",
    "                self.total = len(iterable) if iterable is not None else 0\n",
    "                self.n = 0\n",
    "                self.desc = kwargs.get('desc', '')\n",
    "            \n",
    "            def __iter__(self):\n",
    "                for obj in self.iterable:\n",
    "                    yield obj\n",
    "                    self.n += 1\n",
    "                    # 10%ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "                    if self.total > 0 and self.n % max(1, self.total // 10) == 0:\n",
    "                        print(f\"{self.desc}: {self.n}/{self.total} ({self.n/self.total*100:.1f}%)\")\n",
    "            \n",
    "            def update(self, n=1):\n",
    "                self.n += n\n",
    "            \n",
    "            def close(self):\n",
    "                pass\n",
    "            \n",
    "            def __enter__(self):\n",
    "                return self\n",
    "            \n",
    "            def __exit__(self, *args, **kwargs):\n",
    "                self.close()\n",
    "        \n",
    "        print(\"tqdmì„ ë¡œë“œí•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì§„í–‰ í‘œì‹œê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        return DummyTqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f353de6-8a07-4106-b368-5957b263af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ë¨?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee6554-f8b9-42ab-b77e-e198be0c183a",
   "metadata": {},
   "source": [
    "## 2. ì„¤ì • ë° ê²½ë¡œ ì§€ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2db6b5-88e3-46f2-bb03-77558f7b0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "CHROMA_PERSIST_DIR = \"chroma_db\"  # ChromaDB ì €ì¥ ê²½ë¡œ\n",
    "REVIEW_CSV_PATH = \"bgg-26m-reviews.csv\"   # ë¦¬ë·° CSV íŒŒì¼ ê²½ë¡œ\n",
    "METADATA_CSV_PATH = \"boardgames_combined.csv\"  # ë©”íƒ€ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ None)\n",
    "\n",
    "MIN_REVIEW_LENGTH = 20  # ìµœì†Œ ë¦¬ë·° ê¸¸ì´ (ë„ˆë¬´ ì§§ì€ ë¦¬ë·° í•„í„°ë§)\n",
    "BATCH_SIZE = 1000  # ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°\n",
    "MODEL_NAME = \"intfloat/e5-base-v2\"  # í¬ë¡œìŠ¤ ì–¸ì–´ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸\n",
    "ENRICH_TEXT = True  # ë¦¬ë·°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ë²¡í„°í™”\n",
    "MAX_CHUNK_SIZE = 512  # ìµœëŒ€ ì²­í¬ í¬ê¸°\n",
    "SAMPLE_SIZE = None  # ìƒ˜í”Œë§ í¬ê¸° (Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n",
    "\n",
    "# ID ì»¬ëŸ¼ ì„¤ì • (ëª…ì‹œì  ì§€ì •) - ìˆ˜ì • ë²„ì „\n",
    "REVIEW_ID_COLUMN = \"ID\"  # ë¦¬ë·° ë°ì´í„°ì˜ ID ì»¬ëŸ¼ ëª…ì‹œì  ì§€ì •\n",
    "METADATA_ID_COLUMN = \"Game_Id\"  # ë©”íƒ€ë°ì´í„°ì˜ ID ì»¬ëŸ¼ ëª…ì‹œì  ì§€ì • (idê°€ ì•„ë‹Œ Game_Idë¡œ ìˆ˜ì •)\n",
    "\n",
    "# ì¤‘ìš” ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ì˜ ìˆ˜ì •\n",
    "IMPORTANT_META_FIELDS = [\n",
    "    'Game_Id', 'Title', 'Description', 'description_detail', 'AvgRating',\n",
    "    'minplayers', 'maxplayers', 'playingtime', 'minage', \n",
    "    'category_bert', 'CategoryType', 'boardgamemechanic', \n",
    "]\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ì‚¬ìš© ì¥ì¹˜: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339da4aa-ce51-4e9a-9d3a-0dfb6402e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "def validate_environment_settings():\n",
    "    \"\"\"í™˜ê²½ ì„¤ì •ì„ ê²€ì¦í•˜ê³  ìƒì„¸ ì •ë³´ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ” í™˜ê²½ ì„¤ì • ê²€ì¦ ì‹œì‘\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    validation_results = {\n",
    "        \"success\": 0,\n",
    "        \"warning\": 0,\n",
    "        \"error\": 0\n",
    "    }\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´:\")\n",
    "    print(f\"   - Python ë²„ì „: {sys.version.split()[0]}\")\n",
    "    print(f\"   - ìš´ì˜ì²´ì œ: {sys.platform}\")\n",
    "    print(f\"   - í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "    \n",
    "    # CHROMA_PERSIST_DIR ê²€ì¦\n",
    "    print(f\"\\nğŸ“ ChromaDB ì €ì¥ ê²½ë¡œ: {CHROMA_PERSIST_DIR}\")\n",
    "    if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "        print(f\"   âœ… ê²½ë¡œê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "        print(f\"   - ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(CHROMA_PERSIST_DIR)}\")\n",
    "        print(f\"   - ê¸°ì¡´ ë°ì´í„°ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        validation_results[\"success\"] += 1\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.\")\n",
    "        # ì“°ê¸° ê¶Œí•œ í™•ì¸\n",
    "        try:\n",
    "            parent_dir = os.path.dirname(CHROMA_PERSIST_DIR) or \".\"\n",
    "            if os.access(parent_dir, os.W_OK):\n",
    "                print(f\"   âœ… ë””ë ‰í† ë¦¬ ìƒì„± ê¶Œí•œì´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "                validation_results[\"success\"] += 1\n",
    "            else:\n",
    "                print(f\"   âŒ ë””ë ‰í† ë¦¬ ìƒì„± ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë³€ê²½í•˜ì„¸ìš”.\")\n",
    "                validation_results[\"error\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ê¶Œí•œ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            validation_results[\"error\"] += 1\n",
    "    \n",
    "    # ë¦¬ë·° CSV íŒŒì¼ ê²€ì¦\n",
    "    print(f\"\\nğŸ“„ ë¦¬ë·° CSV íŒŒì¼: {REVIEW_CSV_PATH}\")\n",
    "    if REVIEW_CSV_PATH and os.path.exists(REVIEW_CSV_PATH):\n",
    "        file_size = os.path.getsize(REVIEW_CSV_PATH) / (1024 * 1024)  # MB ë‹¨ìœ„ë¡œ ë³€í™˜\n",
    "        print(f\"   âœ… íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "        print(f\"   - íŒŒì¼ í¬ê¸°: {file_size:.2f} MB\")\n",
    "        print(f\"   - ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(REVIEW_CSV_PATH)}\")\n",
    "        # íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ì²« ì¤„)\n",
    "        try:\n",
    "            with open(REVIEW_CSV_PATH, 'r', encoding='utf-8') as f:\n",
    "                first_line = f.readline().strip()\n",
    "                print(f\"   - ì²« ì¤„ ë¯¸ë¦¬ë³´ê¸°: {first_line[:100]}{'...' if len(first_line) > 100 else ''}\")\n",
    "            validation_results[\"success\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "            print(f\"   - UTF-8 ì¸ì½”ë”©ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì½”ë“œì—ì„œ ë‹¤ì–‘í•œ ì¸ì½”ë”©ì„ ì‹œë„í•©ë‹ˆë‹¤.\")\n",
    "            validation_results[\"warning\"] += 1\n",
    "    else:\n",
    "        print(f\"   âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        validation_results[\"error\"] += 1\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° CSV íŒŒì¼ ê²€ì¦\n",
    "    print(f\"\\nğŸ“„ ë©”íƒ€ë°ì´í„° CSV íŒŒì¼: {METADATA_CSV_PATH}\")\n",
    "    if METADATA_CSV_PATH and os.path.exists(METADATA_CSV_PATH):\n",
    "        file_size = os.path.getsize(METADATA_CSV_PATH) / (1024 * 1024)  # MB ë‹¨ìœ„ë¡œ ë³€í™˜\n",
    "        print(f\"   âœ… íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "        print(f\"   - íŒŒì¼ í¬ê¸°: {file_size:.2f} MB\")\n",
    "        print(f\"   - ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(METADATA_CSV_PATH)}\")\n",
    "        # íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ì²« ì¤„)\n",
    "        try:\n",
    "            with open(METADATA_CSV_PATH, 'r', encoding='utf-8') as f:\n",
    "                first_line = f.readline().strip()\n",
    "                print(f\"   - ì²« ì¤„ ë¯¸ë¦¬ë³´ê¸°: {first_line[:100]}{'...' if len(first_line) > 100 else ''}\")\n",
    "            validation_results[\"success\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "            print(f\"   - UTF-8 ì¸ì½”ë”©ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì½”ë“œì—ì„œ ë‹¤ì–‘í•œ ì¸ì½”ë”©ì„ ì‹œë„í•©ë‹ˆë‹¤.\")\n",
    "            validation_results[\"warning\"] += 1\n",
    "    else:\n",
    "        print(f\"   âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"   - ë©”íƒ€ë°ì´í„° ì—†ì´ ë¦¬ë·° ë°ì´í„°ë§Œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\")\n",
    "        validation_results[\"warning\"] += 1\n",
    "    \n",
    "    # ê¸°íƒ€ ì„¤ì • ê²€ì¦\n",
    "    print(f\"\\nâš™ï¸ ê¸°íƒ€ ì„¤ì • ê²€ì¦:\")\n",
    "    print(f\"   - ìµœì†Œ ë¦¬ë·° ê¸¸ì´: {MIN_REVIEW_LENGTH} ê¸€ì\")\n",
    "    if MIN_REVIEW_LENGTH < 10:\n",
    "        print(f\"   âš ï¸ ìµœì†Œ ë¦¬ë·° ê¸¸ì´ê°€ ì§§ìŠµë‹ˆë‹¤. ë„ˆë¬´ ì§§ì€ ë¦¬ë·°ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        validation_results[\"warning\"] += 1\n",
    "    else:\n",
    "        print(f\"   âœ… ìµœì†Œ ë¦¬ë·° ê¸¸ì´ê°€ ì ì ˆí•©ë‹ˆë‹¤.\")\n",
    "        validation_results[\"success\"] += 1\n",
    "    \n",
    "    print(f\"   - ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°: {BATCH_SIZE}\")\n",
    "    if BATCH_SIZE < 100:\n",
    "        print(f\"   âš ï¸ ë°°ì¹˜ í¬ê¸°ê°€ ì‘ìŠµë‹ˆë‹¤. ì²˜ë¦¬ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        validation_results[\"warning\"] += 1\n",
    "    elif BATCH_SIZE > 5000:\n",
    "        print(f\"   âš ï¸ ë°°ì¹˜ í¬ê¸°ê°€ í½ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        validation_results[\"warning\"] += 1\n",
    "    else:\n",
    "        print(f\"   âœ… ë°°ì¹˜ í¬ê¸°ê°€ ì ì ˆí•©ë‹ˆë‹¤.\")\n",
    "        validation_results[\"success\"] += 1\n",
    "    \n",
    "    print(f\"   - ì„ë² ë”© ëª¨ë¸: {MODEL_NAME}\")\n",
    "    # ì˜¨ë¼ì¸ ì—°ê²° í™•ì¸ (HF ëª¨ë¸ì— ì ‘ê·¼ ê°€ëŠ¥í•œì§€)\n",
    "    print(f\"   âš ï¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "    validation_results[\"warning\"] += 1\n",
    "    \n",
    "    print(f\"   - ìµœëŒ€ ì²­í¬ í¬ê¸°: {MAX_CHUNK_SIZE}\")\n",
    "    if MAX_CHUNK_SIZE < 256:\n",
    "        print(f\"   âš ï¸ ì²­í¬ í¬ê¸°ê°€ ì‘ìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ê°€ ë¶ˆì¶©ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        validation_results[\"warning\"] += 1\n",
    "    elif MAX_CHUNK_SIZE > 1024:\n",
    "        print(f\"   âš ï¸ ì²­í¬ í¬ê¸°ê°€ í½ë‹ˆë‹¤. ì²˜ë¦¬ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        validation_results[\"warning\"] += 1\n",
    "    else:\n",
    "        print(f\"   âœ… ì²­í¬ í¬ê¸°ê°€ ì ì ˆí•©ë‹ˆë‹¤.\")\n",
    "        validation_results[\"success\"] += 1\n",
    "    \n",
    "    print(f\"   - ë¦¬ë·°ì™€ ë©”íƒ€ë°ì´í„° ê²°í•©: {'ì˜ˆ' if ENRICH_TEXT else 'ì•„ë‹ˆì˜¤'}\")\n",
    "    if ENRICH_TEXT:\n",
    "        print(f\"   âœ… ë¦¬ë·°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ë” í’ë¶€í•œ ë²¡í„° ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        validation_results[\"success\"] += 1\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ë¦¬ë·° ë°ì´í„°ë§Œ ë²¡í„°í™”í•©ë‹ˆë‹¤. ë©”íƒ€ë°ì´í„° ì •ë³´ê°€ í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        validation_results[\"warning\"] += 1\n",
    "    \n",
    "    # ID ì»¬ëŸ¼ ì„¤ì • ê²€ì¦\n",
    "    print(f\"\\nğŸ”‘ ID ì»¬ëŸ¼ ì„¤ì •:\")\n",
    "    print(f\"   - ë¦¬ë·° ë°ì´í„° ID ì»¬ëŸ¼: {REVIEW_ID_COLUMN}\")\n",
    "    print(f\"   - ë©”íƒ€ë°ì´í„° ID ì»¬ëŸ¼: {METADATA_ID_COLUMN}\")\n",
    "    print(f\"   âš ï¸ ID ì»¬ëŸ¼ì€ ì‹¤ì œ íŒŒì¼ì—ì„œ ë‹¤ì‹œ í™•ì¸ë©ë‹ˆë‹¤.\")\n",
    "    validation_results[\"warning\"] += 1\n",
    "    \n",
    "    # CUDA ê°€ìš©ì„± ê²€ì‚¬\n",
    "    print(f\"\\nğŸ–¥ï¸ í•˜ë“œì›¨ì–´ ê°€ì† ìƒíƒœ:\")\n",
    "    print(f\"   - ì„ íƒëœ ì¥ì¹˜: {device}\")\n",
    "    if device == \"cuda\":\n",
    "        print(f\"   âœ… CUDA ì‚¬ìš© ê°€ëŠ¥! GPU ê°€ì†ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\")\n",
    "        # CUDA ìƒì„¸ ì •ë³´\n",
    "        cuda_available = torch.cuda.is_available()\n",
    "        device_count = torch.cuda.device_count() if cuda_available else 0\n",
    "        current_device = torch.cuda.current_device() if cuda_available else -1\n",
    "        device_name = torch.cuda.get_device_name(current_device) if cuda_available and device_count > 0 else \"N/A\"\n",
    "        \n",
    "        print(f\"   - CUDA ë²„ì „: {torch.version.cuda if hasattr(torch.version, 'cuda') else 'N/A'}\")\n",
    "        print(f\"   - GPU ê°œìˆ˜: {device_count}\")\n",
    "        print(f\"   - í˜„ì¬ GPU: {device_name}\")\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥ ì‹œë„\n",
    "        try:\n",
    "            if cuda_available and device_count > 0:\n",
    "                free_mem, total_mem = torch.cuda.mem_get_info(current_device)\n",
    "                free_mem_gb = free_mem / (1024**3)\n",
    "                total_mem_gb = total_mem / (1024**3)\n",
    "                print(f\"   - GPU ë©”ëª¨ë¦¬: {free_mem_gb:.2f}GB ê°€ìš© / {total_mem_gb:.2f}GB ì „ì²´\")\n",
    "        except:\n",
    "            print(f\"   - GPU ë©”ëª¨ë¦¬ ì •ë³´: í™•ì¸ ë¶ˆê°€\")\n",
    "        \n",
    "        validation_results[\"success\"] += 1\n",
    "    else:\n",
    "        print(f\"   âš ï¸ CPUë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        validation_results[\"warning\"] += 1\n",
    "    \n",
    "    # ì´í‰\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ğŸ“Š í™˜ê²½ ì„¤ì • ê²€ì¦ ê²°ê³¼:\")\n",
    "    print(f\"   âœ… ì„±ê³µ: {validation_results['success']}ê°œ\")\n",
    "    print(f\"   âš ï¸ ê²½ê³ : {validation_results['warning']}ê°œ\")\n",
    "    print(f\"   âŒ ì˜¤ë¥˜: {validation_results['error']}ê°œ\")\n",
    "    \n",
    "    if validation_results[\"error\"] > 0:\n",
    "        print(\"\\nâ›” ì‹¬ê°í•œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ ì˜¤ë¥˜ë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")\n",
    "    elif validation_results[\"warning\"] > 0:\n",
    "        print(\"\\nâš ï¸ ì¼ë¶€ ê²½ê³ ê°€ ìˆì§€ë§Œ ì‹¤í–‰ì€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ìœ„ ê²½ê³ ë¥¼ ê²€í† í•˜ì„¸ìš”.\")\n",
    "    else:\n",
    "        print(\"\\nâœ… ëª¨ë“  ì„¤ì •ì´ ìœ íš¨í•©ë‹ˆë‹¤. ì‹¤í–‰ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return validation_results[\"error\"] == 0  # ì˜¤ë¥˜ê°€ ì—†ìœ¼ë©´ True ë°˜í™˜\n",
    "\n",
    "# ê²€ì¦ í•¨ìˆ˜ ì‹¤í–‰\n",
    "is_valid = validate_environment_settings()\n",
    "if is_valid:\n",
    "    print(\"í™˜ê²½ ì„¤ì •ì´ ìœ íš¨í•©ë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"í™˜ê²½ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ìœ„ì˜ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c67a6c-2c01-47b3-a2ce-47d3e409ab71",
   "metadata": {},
   "source": [
    "## 3. ë©”íƒ€ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0dd98f-a805-432c-9ac3-dbc5fc1b1f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0027bb70-655c-4093-a20e-c51091d678c5",
   "metadata": {},
   "source": [
    "## 4. CSV ë¦¬ë·° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02e2bd-b147-4725-803f-423c8b28935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys # sys import ì¶”ê°€\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed # as_completed ì¶”ê°€\n",
    "import ast # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±ì„ ìœ„í•´ ì¶”ê°€\n",
    "\n",
    "# tqdm ì„¤ì • (ì½˜ì†” ë˜ëŠ” ë…¸íŠ¸ë¶ í™˜ê²½ ìë™ ê°ì§€ ì‹œë„)\n",
    "try:\n",
    "    # 'get_ipython'ì— ëŒ€í•œ ì •ì˜ ì˜¤ë¥˜ ë°©ì§€\n",
    "    ipython_shell = get_ipython()\n",
    "    shell = ipython_shell.__class__.__name__\n",
    "    if shell == 'ZMQInteractiveShell':\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook # Jupyter í™˜ê²½\n",
    "    else:\n",
    "        from tqdm import tqdm as tqdm_notebook # ë‹¤ë¥¸ IPython í™˜ê²½ (ì˜ˆ: í„°ë¯¸ë„)\n",
    "except NameError:\n",
    "    from tqdm import tqdm as tqdm_notebook # IPythonì´ ì•„ë‹Œ í™˜ê²½\n",
    "\n",
    "# ê¸°ì¡´ ì„í¬íŠ¸ ìœ ì§€\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- ì„¤ì • ë³€ìˆ˜ ---\n",
    "CHROMA_PERSIST_DIR = \"./chroma_db_integrated4\" # ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½ ê°€ëŠ¥\n",
    "REVIEW_CSV_PATH = \"bgg-26m-reviews.csv\"\n",
    "METADATA_CSV_PATH = \"boardgames_combined.csv\"\n",
    "MIN_REVIEW_LENGTH = 20\n",
    "BATCH_SIZE = 500 # add_documents_to_vectorstore ì—ì„œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-small\"\n",
    "ENRICH_TEXT = True\n",
    "MAX_CHUNK_SIZE = 512\n",
    "SAMPLE_SIZE = None # Noneì´ë©´ ì „ì²´ ë°ì´í„° ì²˜ë¦¬\n",
    "CHECKPOINT_DIR = \"./checkpoints4\" # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "\n",
    "REVIEW_ID_COLUMN = \"ID\" # ë¦¬ë·° íŒŒì¼ì˜ ê²Œì„ ID ì»¬ëŸ¼ëª…\n",
    "METADATA_ID_COLUMN = \"Game_Id\" # ë©”íƒ€ë°ì´í„° íŒŒì¼ì˜ ê²Œì„ ID ì»¬ëŸ¼ëª…\n",
    "\n",
    "IMPORTANT_META_FIELDS = [\n",
    "    'Game_Id', 'Title', 'Description', 'description_detail', 'AvgRating',\n",
    "    'minplayers', 'maxplayers', 'playingtime', 'minage',\n",
    "    'category_bert', 'CategoryType', 'boardgamemechanic',\n",
    "    'averageweight'\n",
    "]\n",
    "\n",
    "# --- E5 ëª¨ë¸ìš© í¬ë§· í•¨ìˆ˜ ---\n",
    "def format_query(user_query):\n",
    "    \"\"\"E5 ëª¨ë¸ìš© ì¿¼ë¦¬ í¬ë§·íŒ…\"\"\"\n",
    "    return f\"query: {user_query.strip()}\"\n",
    "\n",
    "def format_passage(doc_text):\n",
    "    \"\"\"E5 ëª¨ë¸ìš© ë¬¸ì„œ(Passage) í¬ë§·íŒ…\"\"\"\n",
    "    return f\"passage: {doc_text.strip()}\"\n",
    "\n",
    "\n",
    "# --- 1. GPU ì§„ë‹¨ ë° ì„¤ì • í•¨ìˆ˜ ---\n",
    "def check_gpu_status():\n",
    "    \"\"\"GPU ìƒíƒœë¥¼ ìì„¸íˆ ì§„ë‹¨í•˜ê³  ë¬¸ì œì ì„ ë³´ê³ í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"\\n=== GPU ì§„ë‹¨ ì‹œì‘ ===\")\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {cuda_available}\")\n",
    "\n",
    "    if cuda_available:\n",
    "        print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "        print(f\"GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            try:\n",
    "                free_mem, total_mem = torch.cuda.mem_get_info(i)\n",
    "                free_mem_gb = free_mem / (1024**3)\n",
    "                total_mem_gb = total_mem / (1024**3)\n",
    "                print(f\"  GPU {i} ë©”ëª¨ë¦¬: {free_mem_gb:.2f}GB ê°€ìš© / {total_mem_gb:.2f}GB ì „ì²´\")\n",
    "            except Exception as e:\n",
    "                print(f\"  GPU {i} ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\")\n",
    "    else:\n",
    "        print(\"CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        # (ì¶”ê°€ì ì¸ ì›ì¸ ë¶„ì„ ë¡œì§ì€ í•„ìš” ì‹œ ì›ë˜ ì½”ë“œì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ)\n",
    "    print(\"=== GPU ì§„ë‹¨ ì™„ë£Œ ===\")\n",
    "    return cuda_available\n",
    "\n",
    "# --- 2. í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì„¤ì • í•¨ìˆ˜ ---\n",
    "def setup_progress_bar():\n",
    "    \"\"\"í™˜ê²½ì— ë§ëŠ” tqdm ì§„í–‰ í‘œì‹œì¤„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"\\n=== í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì„¤ì • ì‹œì‘ ===\")\n",
    "    progress_bar = tqdm_notebook\n",
    "    try:\n",
    "        ipython_shell = get_ipython()\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            print(\"Jupyter Notebook í™˜ê²½ ê°ì§€. tqdm.notebook ì‚¬ìš©.\")\n",
    "            from tqdm.notebook import tqdm as progress_bar\n",
    "            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
    "            try:\n",
    "                for _ in progress_bar(range(2), desc=\"í”„ë¡œê·¸ë ˆìŠ¤ ë°” í…ŒìŠ¤íŠ¸\", leave=False):\n",
    "                    time.sleep(0.01)\n",
    "                print(\"âœ… ë…¸íŠ¸ë¶ í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì‘ë™ í™•ì¸.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"âš ï¸ ë…¸íŠ¸ë¶ í”„ë¡œê·¸ë ˆìŠ¤ ë°” í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}. í‘œì¤€ tqdm ì‚¬ìš©.\")\n",
    "                 from tqdm import tqdm as progress_bar\n",
    "            return progress_bar\n",
    "        else: # ë‹¤ë¥¸ IPython í™˜ê²½ (í„°ë¯¸ë„ ë“±)\n",
    "             print(\"IPython í„°ë¯¸ë„ í™˜ê²½ ê°ì§€. í‘œì¤€ tqdm ì‚¬ìš©.\")\n",
    "             from tqdm import tqdm as progress_bar\n",
    "    except NameError:\n",
    "        print(\"í‘œì¤€ Python í™˜ê²½ ê°ì§€. í‘œì¤€ tqdm ì‚¬ìš©.\")\n",
    "        from tqdm import tqdm as progress_bar\n",
    "    finally:\n",
    "        print(\"=== í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì„¤ì • ì™„ë£Œ ===\")\n",
    "        return progress_bar # ìµœì¢… ê²°ì •ëœ progress_bar ë°˜í™˜\n",
    "        \n",
    "# --- 3. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ ---\n",
    "class CheckpointManager:\n",
    "    def __init__(self, pipeline_id=\"default\"):\n",
    "        self.pipeline_id = pipeline_id\n",
    "        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "        self.checkpoint_file = os.path.join(CHECKPOINT_DIR, f\"{pipeline_id}_checkpoint.json\")\n",
    "        self.state = self._load_state()\n",
    "        # Load ì‹œ setìœ¼ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "        if isinstance(self.state.get(\"processed_ids\", {}).get(\"review_ids\"), list) or \\\n",
    "           isinstance(self.state.get(\"processed_ids\", {}).get(\"document_ids\"), list):\n",
    "             self._convert_ids_to_set()\n",
    "\n",
    "    def _convert_ids_to_set(self):\n",
    "        \"\"\"ë¡œë“œëœ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì§‘í•©(set)ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        if \"processed_ids\" in self.state:\n",
    "            for id_type in self.state[\"processed_ids\"]:\n",
    "                 if isinstance(self.state[\"processed_ids\"][id_type], list):\n",
    "                      # print(f\"Converting processed_ids['{id_type}'] to set...\") # ë””ë²„ê¹…ìš©\n",
    "                      self.state[\"processed_ids\"][id_type] = set(self.state[\"processed_ids\"][id_type])\n",
    "\n",
    "    def _load_state(self):\n",
    "        \"\"\"ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ë¡œë“œ\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:\n",
    "                    loaded_state = json.load(f)\n",
    "                # ë¡œë“œ í›„ ë°”ë¡œ setìœ¼ë¡œ ë³€í™˜\n",
    "                if \"processed_ids\" in loaded_state:\n",
    "                    for id_type in loaded_state[\"processed_ids\"]:\n",
    "                        if isinstance(loaded_state[\"processed_ids\"][id_type], list):\n",
    "                             loaded_state[\"processed_ids\"][id_type] = set(loaded_state[\"processed_ids\"][id_type])\n",
    "                return loaded_state\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ JSON ë””ì½”ë”© ì˜¤ë¥˜ ({self.checkpoint_file}): {e}\")\n",
    "                print(\"   -> íŒŒì¼ì„ ë°±ì—…í•˜ê³  ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "                self._backup_corrupted_checkpoint()\n",
    "                return self._init_state()\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ({self.checkpoint_file}): {str(e)}\")\n",
    "                self._backup_corrupted_checkpoint()\n",
    "                return self._init_state()\n",
    "                \n",
    "        else:\n",
    "            return self._init_state()\n",
    "\n",
    "    def _backup_corrupted_checkpoint(self):\n",
    "        \"\"\"ì†ìƒëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ë°±ì—…\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            backup_file = f\"{self.checkpoint_file}.corrupted_{int(time.time())}.bak\"\n",
    "            try : \n",
    "                os.rename(self.checkpoint_file, backup_file)\n",
    "                print(f\"   ì†ìƒëœ íŒŒì¼ ë°±ì—…ë¨: {backup_file}\")\n",
    "            except Exception as backup_e:\n",
    "                print(f\"   âš ï¸ ì†ìƒëœ íŒŒì¼ ë°±ì—… ì‹¤íŒ¨: {backup_e}\")\n",
    "            \n",
    "\n",
    "    def _init_state(self):\n",
    "        \"\"\"ìƒˆ ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ì´ˆê¸°í™”\"\"\"\n",
    "        print(f\"ìƒˆ ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ì´ˆê¸°í™”: {self.pipeline_id}\")\n",
    "        return {\n",
    "            \"pipeline_id\": self.pipeline_id,\n",
    "            \"start_time\": time.time(),\n",
    "            \"last_update\": time.time(),\n",
    "            \"stages\": {\n",
    "                # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ì— ë§ì¶° ìˆ˜ì •\n",
    "                \"metadata_loaded\": False, \"reviews_processed\": False, \"data_enriched\": False,\n",
    "                \"documents_created\": False, \"documents_split\": False, \"vectorstore_setup\": False,\n",
    "                \"passage_formatting_applied\": False, # E5 í¬ë§·íŒ… ë‹¨ê³„ ì¶”ê°€\n",
    "                \"documents_added\": False, \"vectorstore_tested\": False\n",
    "            },\n",
    "            \"counters\": {\n",
    "                \"metadata_count\": 0, \"reviews_processed\": 0, \"enriched_count\": 0,\n",
    "                \"documents_created\": 0, \"documents_split\": 0, \"documents_added\": 0,\n",
    "                \"last_added_batch_index\": -1\n",
    "            },\n",
    "            \"processed_ids\": {\n",
    "                \"review_ids\": set(), # ë¦¬ë·° ì²˜ë¦¬ ì¤‘ë³µ ë°©ì§€ìš©\n",
    "                \"document_ids\": set() # ë²¡í„° ì €ì¥ì†Œ ì¶”ê°€ ì¤‘ë³µ ë°©ì§€ìš© (Chroma ID ê¸°ì¤€)\n",
    "            },\n",
    "            # ì¶”ê°€ ì •ë³´ ì €ì¥ ê°€ëŠ¥ (ì˜ˆ: ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ)\n",
    "            \"source_files\": {\n",
    "                \"metadata_file\": None,\n",
    "                \"review_file\": None\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"í˜„ì¬ ìƒíƒœë¥¼ íŒŒì¼ì— ì €ì¥ (setì„ listë¡œ ë³€í™˜)\"\"\"\n",
    "        save_state = self.state.copy()\n",
    "        # JSON ì €ì¥ì„ ìœ„í•´ setì„ listë¡œ ë³€í™˜\n",
    "        save_state[\"processed_ids\"] = {\n",
    "            key: list(val) for key, val in self.state[\"processed_ids\"].items()\n",
    "        }\n",
    "        save_state[\"last_update\"] = time.time() # ì €ì¥ ì‹œì  ì—…ë°ì´íŠ¸\n",
    "\n",
    "        try:\n",
    "            # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì €ì¥\n",
    "            temp_file = self.checkpoint_file + \".tmp\"\n",
    "            with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(save_state, f, indent=4, ensure_ascii=False)\n",
    "            # ì €ì¥ ì„±ê³µ ì‹œ ì›ë³¸ íŒŒì¼ë¡œ ë³€ê²½ (ì›ìì  ì—°ì‚° ì‹œë„)\n",
    "            os.replace(temp_file, self.checkpoint_file)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì˜¤ë¥˜ ({self.checkpoint_file}): {e}\")\n",
    "            # ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹œë„\n",
    "            if os.path.exists(temp_file):\n",
    "                 try: os.remove(temp_file)\n",
    "                 except: pass\n",
    "\n",
    "    def update_stage(self, stage, status=True):\n",
    "        \"\"\"ë‹¨ê³„ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        if stage in self.state[\"stages\"]:\n",
    "            # ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì €ì¥\n",
    "            if self.state[\"stages\"][stage] != status:\n",
    "                 self.state[\"stages\"][stage] = status\n",
    "                 self.save()\n",
    "        else:\n",
    "             print(f\"âš ï¸ ê²½ê³ : ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì²´í¬í¬ì¸íŠ¸ ë‹¨ê³„ ì—…ë°ì´íŠ¸ ì‹œë„ - {stage}\")\n",
    "\n",
    "    def update_counter(self, counter, value):\n",
    "        \"\"\"ì¹´ìš´í„° ê°’ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        if counter in self.state[\"counters\"]:\n",
    "            # ê°’ì´ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì €ì¥\n",
    "            if self.state[\"counters\"][counter] != value:\n",
    "                 self.state[\"counters\"][counter] = value\n",
    "                 self.save()\n",
    "        else:\n",
    "             print(f\"âš ï¸ ê²½ê³ : ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì²´í¬í¬ì¸íŠ¸ ì¹´ìš´í„° ì—…ë°ì´íŠ¸ ì‹œë„ - {counter}\")\n",
    "\n",
    "    # def increment_counter(self, counter, increment=1):\n",
    "    #     \"\"\"ì¹´ìš´í„° ì¦ê°€\"\"\"\n",
    "    #     if counter in self.state[\"counters\"]:\n",
    "    #         self.state[\"counters\"][counter] += increment\n",
    "    #         # ì¦ê°€ëŠ” ë¹ˆë²ˆí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë§¤ë²ˆ saveí•˜ì§€ ì•Šê³ ,\n",
    "    #         # ë‹¨ê³„ ì™„ë£Œ ì‹œ ë˜ëŠ” ì£¼ê¸°ì ìœ¼ë¡œ saveí•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŒ\n",
    "    #         # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì €ì¥\n",
    "    #         self.save()\n",
    "\n",
    "    def is_stage_completed(self, stage):\n",
    "        \"\"\"ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸\"\"\"\n",
    "        return self.state[\"stages\"].get(stage, False)\n",
    "        \n",
    "    def add_processed_ids(self, id_type, ids):\n",
    "        \"\"\"ì²˜ë¦¬ëœ ID ì¶”ê°€ (set ì—…ë°ì´íŠ¸ í›„ ì €ì¥)\"\"\"\n",
    "        if id_type in self.state[\"processed_ids\"]:\n",
    "            if not isinstance(ids, (list, set, tuple)): # tuple ì¶”ê°€\n",
    "                ids = [ids]\n",
    "            # None ê°’ ì œê±° ë° ë¬¸ìì—´ ë³€í™˜\n",
    "            str_ids = {str(id_val) for id_val in ids if id_val is not None}\n",
    "            if not str_ids: return # ì¶”ê°€í•  ìœ íš¨í•œ IDê°€ ì—†ìœ¼ë©´ ë°˜í™˜\n",
    "\n",
    "            initial_len = len(self.state[\"processed_ids\"][id_type])\n",
    "            self.state[\"processed_ids\"][id_type].update(str_ids)\n",
    "            # ì‹¤ì œ IDê°€ ì¶”ê°€ë˜ì—ˆì„ ë•Œë§Œ ì €ì¥\n",
    "            if len(self.state[\"processed_ids\"][id_type]) > initial_len:\n",
    "                self.save()\n",
    "        else:\n",
    "            print(f\"âš ï¸ ê²½ê³ : ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ID íƒ€ì…ì— ID ì¶”ê°€ ì‹œë„ - {id_type}\")\n",
    "\n",
    "    def is_id_processed(self, id_type, id_value):\n",
    "        \"\"\"IDê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸\"\"\"\n",
    "        if id_type in self.state[\"processed_ids\"] and id_value is not None:\n",
    "            return str(id_value) in self.state[\"processed_ids\"][id_type]\n",
    "        return False\n",
    "\n",
    "    def get_processed_ids_count(self, id_type):\n",
    "        \"\"\"ì²˜ë¦¬ëœ ID ê°œìˆ˜ ë°˜í™˜\"\"\"\n",
    "        return len(self.state.get(\"processed_ids\", {}).get(id_type, set()))\n",
    "\n",
    "    def get_resume_info(self):\n",
    "        \"\"\"ì¬ê°œ ì •ë³´ ì–»ê¸°\"\"\"\n",
    "        completed_stages = [s for s, status in self.state.get(\"stages\", {}).items() if status]\n",
    "        import datetime\n",
    "        last_update_ts = self.state.get(\"last_update\", 0)\n",
    "        last_update_str = \"N/A\"\n",
    "        if last_update_ts:\n",
    "            try:\n",
    "                last_update_str = datetime.datetime.fromtimestamp(last_update_ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            except ValueError: # ìœ íš¨í•˜ì§€ ì•Šì€ íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬\n",
    "                pass\n",
    "\n",
    "        counters = self.state.get(\"counters\", {})\n",
    "        last_added_batch_idx = counters.get(\"last_added_batch_index\", -1)\n",
    "\n",
    "        return {\n",
    "            \"can_resume\": len(completed_stages) > 0 or last_added_batch_idx >= 0,\n",
    "            \"completed_stages\": completed_stages,\n",
    "            \"counters\": counters,\n",
    "            \"processed_counts\": {\n",
    "                \"reviews\": self.get_processed_ids_count(\"review_ids\"),\n",
    "                \"documents\": self.get_processed_ids_count(\"document_ids\"),\n",
    "            },\n",
    "            \"last_update\": last_update_str,\n",
    "            \"last_added_batch_index\": last_added_batch_idx\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ì´ˆê¸°í™”\"\"\"\n",
    "        print(f\"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™” ì¤‘: {self.checkpoint_file}\")\n",
    "        self.state = self._init_state()\n",
    "        self.save()\n",
    "        print(\"âœ… ì²´í¬í¬ì¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def update_source_file(self, file_type, file_path):\n",
    "        \"\"\"ì†ŒìŠ¤ íŒŒì¼ ê²½ë¡œ ì €ì¥\"\"\"\n",
    "        if file_type in self.state.get(\"source_files\", {}):\n",
    "             if self.state[\"source_files\"][file_type] != file_path:\n",
    "                  self.state[\"source_files\"][file_type] = file_path\n",
    "                  self.save()\n",
    "\n",
    "\n",
    "# --- 4. ë³‘ë ¬ ì²˜ë¦¬ ê´€ë¦¬ì ---\n",
    "class ParallelProcessor:\n",
    "    def __init__(self, use_processes=False, max_workers=None):\n",
    "        self.use_processes = use_processes\n",
    "        cpu_count = multiprocessing.cpu_count()\n",
    "        self.max_workers = max_workers if max_workers is not None else max(1, cpu_count - 1)\n",
    "        print(f\"ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •: {'í”„ë¡œì„¸ìŠ¤' if use_processes else 'ìŠ¤ë ˆë“œ'} ëª¨ë“œ, {self.max_workers}ê°œ ì›Œì»¤\")\n",
    "        self.executor_class = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor\n",
    "\n",
    "    def process_batch(self, func, items, desc=\"ë³‘ë ¬ ì²˜ë¦¬ ì¤‘\", chunksize=1, **kwargs):\n",
    "        \"\"\"í•­ëª© ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)\"\"\"\n",
    "        if not items: return []\n",
    "\n",
    "        if len(items) < self.max_workers * 2:\n",
    "             print(f\"â„¹ï¸ í•­ëª© ìˆ˜ê°€ ì ì–´ ({len(items)}ê°œ) ì§ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "             results = []\n",
    "             for item in tqdm_notebook(items, desc=f\"{desc} (ì§ë ¬)\"):\n",
    "                  try:\n",
    "                      results.append(func(item, **kwargs) if kwargs else func(item))\n",
    "                  except Exception as e:\n",
    "                      # ì§ë ¬ ì²˜ë¦¬ ì˜¤ë¥˜ ì‹œ ì¸ë±ìŠ¤ ì •ë³´ ì¶”ê°€\n",
    "                      item_info = f\"í•­ëª© {items.index(item)}\" if isinstance(items, list) else \"í•­ëª©\"\n",
    "                      print(f\"âš ï¸ {item_info} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì§ë ¬): {e}\")\n",
    "                      results.append(None)\n",
    "             return results\n",
    "\n",
    "        results = [None] * len(items)\n",
    "        futures_map = {} # Future -> original index ë§¤í•‘\n",
    "\n",
    "        with self.executor_class(max_workers=self.max_workers) as executor:\n",
    "            # submit ì‚¬ìš© ì‹œ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ì „ë‹¬\n",
    "            for i, item in enumerate(items):\n",
    "                 future = executor.submit(func, item, **kwargs) if kwargs else executor.submit(func, item)\n",
    "                 futures_map[future] = i\n",
    "\n",
    "            with tqdm_notebook(total=len(items), desc=desc) as pbar:\n",
    "                for future in as_completed(futures_map):\n",
    "                    original_index = futures_map[future]\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        results[original_index] = result\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ ë³‘ë ¬ ì‘ì—… ì˜¤ë¥˜ (ì›ë³¸ ì¸ë±ìŠ¤ {original_index}): {e}\")\n",
    "                        # ê²°ê³¼ëŠ” Noneìœ¼ë¡œ ìœ ì§€ë¨\n",
    "                    finally:\n",
    "                         pbar.update(1)\n",
    "\n",
    "        error_count = sum(1 for r in results if r is None) # None ê²°ê³¼ ê°œìˆ˜ í™•ì¸\n",
    "        if error_count > 0:\n",
    "            print(f\"âš ï¸ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ {error_count}/{len(items)}ê°œ í•­ëª©ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# --- ê¸°ì¡´ í•¨ìˆ˜ ì •ì˜ (í•„ìš”ì— ë”°ë¼ ë‚´ë¶€ ìˆ˜ì •) ---\n",
    "\n",
    "# load_game_metadata (ì²´í¬í¬ì¸íŠ¸ ë¡œì§ ì¶”ê°€)\n",
    "def load_game_metadata(metadata_file=METADATA_CSV_PATH, checkpoint_mgr=None, progress_bar=tqdm_notebook):\n",
    "    \"\"\"ë³´ë“œê²Œì„ ë©”íƒ€ë°ì´í„° ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ ì§€ì› ì‹œë„)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“š 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ë¡œë“œ\")\n",
    "    print(\"=\"*50)\n",
    "    stage_name = \"metadata_loaded\"\n",
    "    reload_needed = True # ê¸°ë³¸ì ìœ¼ë¡œ ë¦¬ë¡œë“œ í•„ìš”\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ---\n",
    "    # ë©”íƒ€ë°ì´í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ì‹œ ë¡œë“œí•˜ëŠ” ê²ƒì´ ì•ˆì „í•˜ë¯€ë¡œ,\n",
    "    # ì—¬ê¸°ì„œëŠ” ì™„ë£Œ ì—¬ë¶€ë§Œ í™•ì¸í•˜ê³  ì‹¤ì œ ë¡œë“œëŠ” í•­ìƒ ìˆ˜í–‰í•˜ë„ë¡ í•  ìˆ˜ ìˆìŒ.\n",
    "    # ë˜ëŠ”, ë¡œë“œëœ íŒŒì¼ ê²½ë¡œê°€ ê°™ì€ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ ì¶”ê°€ ê°€ëŠ¥.\n",
    "    if checkpoint_mgr:\n",
    "        if checkpoint_mgr.is_stage_completed(stage_name):\n",
    "             loaded_file = checkpoint_mgr.state[\"source_files\"].get(\"metadata_file\")\n",
    "             if loaded_file == metadata_file:\n",
    "                 print(f\"âœ… ë©”íƒ€ë°ì´í„° '{metadata_file}'ëŠ” ì´ì „ ì‹¤í–‰ì—ì„œ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "                 print(f\"   ì €ì¥ëœ ì¹´ìš´íŠ¸: {checkpoint_mgr.state['counters']['metadata_count']}\")\n",
    "                 # ì‹¤ì œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ê¸°ëŠ¥ì€ ì—†ìœ¼ë¯€ë¡œ, ì¬ë¡œë“œí•˜ê±°ë‚˜ ì—¬ê¸°ì„œ None ë°˜í™˜\n",
    "                 # ì—¬ê¸°ì„œëŠ” ì¬ë¡œë“œ ì§„í–‰\n",
    "                 print(\"   (ì²´í¬í¬ì¸íŠ¸ í™•ì¸í–ˆì§€ë§Œ, ë°ì´í„° ì¬ë¡œë“œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤)\")\n",
    "             else:\n",
    "                  print(f\"âš ï¸ ì´ì „ ë¡œë“œ íŒŒì¼({loaded_file})ê³¼ í˜„ì¬ íŒŒì¼({metadata_file})ì´ ë‹¤ë¦…ë‹ˆë‹¤. ì¬ë¡œë“œí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not os.path.exists(metadata_file):\n",
    "        print(f\"âŒ íŒŒì¼ ì—†ìŒ: {metadata_file}\"); return None, None\n",
    "    # ... (íŒŒì¼ ì •ë³´ ì¶œë ¥, ì¸ì½”ë”© ì‹œë„, ID ì»¬ëŸ¼ ì°¾ê¸°, ë³€í™˜, ì¤‘ë³µ ì œê±° ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ...\n",
    "    file_size = os.path.getsize(metadata_file) / (1024 * 1024)\n",
    "    print(f\"ğŸ“ íŒŒì¼: {metadata_file} ({file_size:.2f} MB)\")\n",
    "    metadata_df = None\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']\n",
    "    for encoding in progress_bar(encodings, desc=\"ë©”íƒ€ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸\"):\n",
    "         try:\n",
    "             metadata_df = pd.read_csv(metadata_file, encoding=encoding)\n",
    "             print(f\"  âœ… {encoding} ë¡œë“œ ì„±ê³µ!\"); break\n",
    "         except: continue\n",
    "    if metadata_df is None: print(\"âŒ ë©”íƒ€ ë¡œë“œ ì‹¤íŒ¨.\"); return None, None\n",
    "\n",
    "    id_column = METADATA_ID_COLUMN if METADATA_ID_COLUMN in metadata_df.columns else None\n",
    "    if not id_column:\n",
    "        id_candidates = ['id', 'ID', 'game_id', 'gameid', 'item_id']\n",
    "        for candidate in id_candidates:\n",
    "             matches = [c for c in metadata_df.columns if c.lower() == candidate.lower()]\n",
    "             if matches: id_column = matches[0]; break\n",
    "    if not id_column: print(\"âš ï¸ ID ì»¬ëŸ¼ ëª»ì°¾ìŒ. ì¸ë±ìŠ¤ ì‚¬ìš©.\"); metadata_df['generated_id'] = metadata_df.index.astype(str); id_column = 'generated_id'\n",
    "    else: print(f\"âœ… ID ì»¬ëŸ¼: '{id_column}'\")\n",
    "\n",
    "    metadata_df[id_column] = metadata_df[id_column].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "    dups = metadata_df.duplicated(subset=[id_column]).sum()\n",
    "    if dups > 0: print(f\"   âš ï¸ ì¤‘ë³µ ID {dups}ê°œ ë°œê²¬. ì²« ê°’ ìœ ì§€.\"); metadata_df = metadata_df.drop_duplicates(subset=[id_column], keep='first')\n",
    "\n",
    "    print(\"ğŸ”„ ì‚¬ì „ ìƒì„± ì¤‘...\")\n",
    "    metadata_dict = metadata_df.set_index(id_column).to_dict('index')\n",
    "    metadata_count = len(metadata_dict)\n",
    "\n",
    "    if checkpoint_mgr:\n",
    "        checkpoint_mgr.update_counter(\"metadata_count\", metadata_count)\n",
    "        checkpoint_mgr.update_stage(stage_name, True)\n",
    "        print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {stage_name}=True, count={metadata_count}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"âœ… ë¡œë“œ ì™„ë£Œ: {metadata_count:,}ê°œ ê²Œì„ ({total_time:.2f}ì´ˆ)\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    return metadata_dict, id_column\n",
    "\n",
    "# preprocess_reviews (ì²´í¬í¬ì¸íŠ¸ ë¡œì§ ì¶”ê°€)\n",
    "def preprocess_reviews(csv_path=REVIEW_CSV_PATH, checkpoint_mgr=None, progress_bar=tqdm_notebook):\n",
    "    \"\"\"CSV ë¦¬ë·° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ì²´í¬í¬ì¸íŠ¸ ì§€ì› ì‹œë„)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“ 2ë‹¨ê³„: ë¦¬ë·° ë°ì´í„° ì²˜ë¦¬\")\n",
    "    print(\"=\"*70)\n",
    "    stage_name = \"reviews_processed\"\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ---\n",
    "    if checkpoint_mgr:\n",
    "         if checkpoint_mgr.is_stage_completed(stage_name):\n",
    "             loaded_file = checkpoint_mgr.state[\"source_files\"].get(\"review_file\")\n",
    "             if loaded_file == csv_path:\n",
    "                  processed_count = checkpoint_mgr.state[\"counters\"][\"reviews_processed\"]\n",
    "                  saved_id_count = checkpoint_mgr.get_processed_ids_count(\"review_ids\")\n",
    "                  print(f\"âœ… ë¦¬ë·° íŒŒì¼ '{csv_path}'ëŠ” ì´ì „ ì‹¤í–‰ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "                  print(f\"   ì €ì¥ëœ ì²˜ë¦¬ ì¹´ìš´íŠ¸: {processed_count:,}, ì €ì¥ëœ ID ìˆ˜: {saved_id_count:,}\")\n",
    "                  # ì—¬ê¸°ì„œ ì²˜ë¦¬ë¥¼ ê±´ë„ˆë›°ë ¤ë©´, ì´ì „ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ëŠ” ë§¤ì»¤ë‹ˆì¦˜ í•„ìš”.\n",
    "                  # ì—¬ê¸°ì„œëŠ” ID ê¸°ë°˜ ì¤‘ë³µ ì²˜ë¦¬ë¥¼ ê°•í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰.\n",
    "                  print(\"   (ì²´í¬í¬ì¸íŠ¸ í™•ì¸. ID ê¸°ë°˜ ì¤‘ë³µ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•©ë‹ˆë‹¤)\")\n",
    "             else:\n",
    "                  print(f\"âš ï¸ ì´ì „ ì²˜ë¦¬ íŒŒì¼({loaded_file})ê³¼ í˜„ì¬ íŒŒì¼({csv_path})ì´ ë‹¤ë¦…ë‹ˆë‹¤. ìƒˆë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "                  # ì´ì „ ID ì •ë³´ê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì´ˆê¸°í™” í•„ìš” ì‹œ ê³ ë ¤\n",
    "                  # checkpoint_mgr.state[\"processed_ids\"][\"review_ids\"] = set()\n",
    "                  # checkpoint_mgr.save()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}\")\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"ğŸ“‚ ë¦¬ë·° ë°ì´í„° íŒŒì¼: {csv_path}\")\n",
    "    # (íŒŒì¼ í¬ê¸° í™•ì¸ ë“± ìœ ì§€)\n",
    "\n",
    "    df = None\n",
    "    try:\n",
    "        # (ì¸ì½”ë”© ì‹œë„ ë¡œì§ ìœ ì§€, SAMPLE_SIZE ì ìš© ìœ ì§€)\n",
    "        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']\n",
    "        encoding_used = None\n",
    "        for encoding in progress_bar(encodings, desc=\"ë¦¬ë·° íŒŒì¼ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸\"):\n",
    "            try:\n",
    "                 print(f\"   ğŸ” {encoding} ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„...\")\n",
    "                 # ìƒ˜í”Œ ë¡œë“œë¡œ ì¸ì½”ë”© í™•ì¸\n",
    "                 pd.read_csv(csv_path, nrows=5, encoding=encoding)\n",
    "                 # ë¡œë“œ\n",
    "                 read_kwargs = {'encoding': encoding, 'on_bad_lines': 'warn'}\n",
    "                 if SAMPLE_SIZE:\n",
    "                      print(f\"\\nâš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì²˜ìŒ {SAMPLE_SIZE:,}í–‰ë§Œ ì½ìŠµë‹ˆë‹¤.\")\n",
    "                      read_kwargs['nrows'] = SAMPLE_SIZE\n",
    "                 else:\n",
    "                      print(f\"\\nğŸ“‚ ì „ì²´ ë°ì´í„° ë¡œë“œ ì‹œë„...\")\n",
    "                 try:\n",
    "                     df = pd.read_csv(csv_path, **read_kwargs)\n",
    "                 except pd.errors.ParserError as pe:\n",
    "                     print(f\"   âš ï¸ íŒŒì‹± ì˜¤ë¥˜ ({encoding}): {pe}. ì˜¤ë¥˜ ë¼ì¸ ê±´ë„ˆë›°ê³  ì¬ì‹œë„.\")\n",
    "                     read_kwargs['on_bad_lines'] = 'skip'\n",
    "                     df = pd.read_csv(csv_path, **read_kwargs)\n",
    "\n",
    "                 print(f\"   âœ… {encoding} ë¡œë“œ ì„±ê³µ.\")\n",
    "                 encoding_used = encoding\n",
    "                 break\n",
    "            except UnicodeDecodeError:\n",
    "                 print(f\"   âŒ {encoding} ì¸ì½”ë”© ì‹¤íŒ¨\")\n",
    "                 continue\n",
    "            except Exception as e:\n",
    "                 print(f\"   âŒ ë¡œë“œ ì˜¤ë¥˜ ({encoding}): {e}\")\n",
    "                 continue\n",
    "\n",
    "        if df is None:\n",
    "            print(\"âŒ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨.\")\n",
    "            if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "            return None, None, None\n",
    "\n",
    "        print(f\"\\nâœ… CSV ë¡œë“œ ì„±ê³µ: {len(df):,}í–‰\")\n",
    "\n",
    "        # (ë¦¬ë·° ì»¬ëŸ¼ ì°¾ê¸°, ID ì»¬ëŸ¼ ì°¾ê¸° ë° ë³€í™˜ ë¡œì§ ìœ ì§€)\n",
    "        review_column_candidates = ['comment', 'review', 'text', 'content', 'review_text']\n",
    "        review_column = None\n",
    "        for col in review_column_candidates:\n",
    "            col_matches = [c for c in df.columns if c.lower() == col.lower()]\n",
    "            if col_matches:\n",
    "                review_column = col_matches[0]; break\n",
    "        if not review_column: print(\"âŒ ë¦¬ë·° ì»¬ëŸ¼ ì°¾ê¸° ì‹¤íŒ¨.\"); return None, None, None\n",
    "        print(f\"âœ… ë¦¬ë·° ì»¬ëŸ¼: '{review_column}'\")\n",
    "\n",
    "        id_column = None\n",
    "        id_candidates = [REVIEW_ID_COLUMN, 'id', 'ID', 'game_id', 'gameid', 'item_id', 'bgg_id', 'gid']\n",
    "        id_candidates = [c for c in id_candidates if c] # None ì œê±°\n",
    "        for candidate in id_candidates:\n",
    "             col_matches = [c for c in df.columns if c.lower() == candidate.lower()]\n",
    "             if col_matches:\n",
    "                 id_column = col_matches[0]; break\n",
    "        if not id_column: print(\"âš ï¸ ë¦¬ë·° ID ì»¬ëŸ¼ ì°¾ê¸° ì‹¤íŒ¨. ë©”íƒ€ë°ì´í„° ì—°ê²° ë¶ˆê°€.\")\n",
    "        else:\n",
    "             print(f\"âœ… ë¦¬ë·° ID ì»¬ëŸ¼: '{id_column}'\")\n",
    "             print(f\"\\nğŸ”„ ID ì»¬ëŸ¼ '{id_column}' ë¬¸ìì—´ ë³€í™˜ ë° .0 ì œê±°...\")\n",
    "             df[id_column] = df[id_column].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "\n",
    "        # --- ë°ì´í„° ì „ì²˜ë¦¬ (ì²´í¬í¬ì¸íŠ¸ ID í™•ì¸ ë¡œì§ ì¶”ê°€) ---\n",
    "        print(\"\\nğŸ”„ ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "        initial_count = len(df)\n",
    "        processed_ids_in_this_run = set()\n",
    "        processed_rows = []\n",
    "\n",
    "        # CheckpointManagerì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ ID ë¡œë“œ\n",
    "        previously_processed_ids = set()\n",
    "        if checkpoint_mgr and id_column:\n",
    "            previously_processed_ids = checkpoint_mgr.state[\"processed_ids\"].get(\"review_ids\", set())\n",
    "            print(f\"   â„¹ï¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œëœ ì´ì „ ì²˜ë¦¬ ë¦¬ë·° ID: {len(previously_processed_ids):,}ê°œ\")\n",
    "\n",
    "        for index, row in progress_bar(df.iterrows(), total=initial_count, desc=\"ë¦¬ë·° ì „ì²˜ë¦¬ ë° í•„í„°ë§\"):\n",
    "             # 1. ID ê¸°ë°˜ ê±´ë„ˆë›°ê¸° (ID ì»¬ëŸ¼ì´ ìˆê³ , ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© ì‹œ)\n",
    "             current_id = None\n",
    "             if id_column:\n",
    "                 current_id = row[id_column]\n",
    "                 if checkpoint_mgr and checkpoint_mgr.is_id_processed(\"review_ids\", current_id):\n",
    "                     continue # ì´ë¯¸ ì²˜ë¦¬ëœ IDë©´ ê±´ë„ˆë›°ê¸°\n",
    "\n",
    "             # 2. Null ë˜ëŠ” ë¹ˆ ë¦¬ë·° ì œê±°\n",
    "             review_text = row[review_column]\n",
    "             if pd.isna(review_text): continue\n",
    "             review_text_str = str(review_text).strip()\n",
    "             if not review_text_str: continue\n",
    "\n",
    "             # 3. ì§§ì€ ë¦¬ë·° ì œê±°\n",
    "             if len(review_text_str) < MIN_REVIEW_LENGTH: continue\n",
    "\n",
    "             # ëª¨ë“  í•„í„° í†µê³¼ ì‹œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "             processed_rows.append(row.to_dict()) # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€\n",
    "\n",
    "             # ì´ë²ˆ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬ëœ ID ê¸°ë¡ (ë‚˜ì¤‘ì— ì¤‘ë³µ ì œê±° ë° ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ìš©)\n",
    "             if current_id is not None:\n",
    "                 processed_ids_in_this_run.add(current_id)\n",
    "\n",
    "        # ì²˜ë¦¬ëœ í–‰ë“¤ë¡œ ìƒˆ DataFrame ìƒì„±\n",
    "        if not processed_rows:\n",
    "             print(\"âš ï¸ ì „ì²˜ë¦¬ í›„ ìœ íš¨í•œ ë¦¬ë·° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "             if checkpoint_mgr:\n",
    "                  # ì²˜ë¦¬ëŠ” í–ˆì§€ë§Œ ê²°ê³¼ê°€ ì—†ìœ¼ë¯€ë¡œ ì™„ë£Œ ìƒíƒœëŠ” ì•„ë‹˜\n",
    "                  checkpoint_mgr.update_stage(stage_name, False)\n",
    "                  checkpoint_mgr.update_counter(\"reviews_processed\", 0)\n",
    "             return None, review_column, id_column\n",
    "\n",
    "        processed_df = pd.DataFrame(processed_rows)\n",
    "        print(f\"âœ… ê¸°ë³¸ í•„í„°ë§ ì™„ë£Œ: {len(processed_df):,}í–‰ ë‚¨ìŒ\")\n",
    "\n",
    "        # 4. ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ì²˜ë¦¬ëœ ê²°ê³¼ì— ëŒ€í•´ ìˆ˜í–‰)\n",
    "        before_dedup = len(processed_df)\n",
    "        processed_df = processed_df.drop_duplicates(subset=[review_column], keep='first')\n",
    "        print(f\"âœ… ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ë¦¬ë·° ì œê±°: {before_dedup - len(processed_df):,}ê°œ ì œê±°ë¨\")\n",
    "\n",
    "        final_count = len(processed_df)\n",
    "        print(f\"\\nğŸ“Š ì „ì²˜ë¦¬ í›„ ìµœì¢… ë°ì´í„° í¬ê¸°: {final_count:,}í–‰\")\n",
    "\n",
    "        # --- ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ ---\n",
    "        if checkpoint_mgr:\n",
    "            # ìƒˆë¡œ ì²˜ë¦¬ëœ IDë“¤ì„ ì²´í¬í¬ì¸íŠ¸ì— ì¶”ê°€\n",
    "            if id_column and processed_ids_in_this_run:\n",
    "                 newly_added_ids = processed_ids_in_this_run - previously_processed_ids\n",
    "                 if newly_added_ids:\n",
    "                     checkpoint_mgr.add_processed_ids(\"review_ids\", list(newly_added_ids))\n",
    "                     print(f\"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ì— ìƒˆë¡œ ì²˜ë¦¬ëœ ë¦¬ë·° ID {len(newly_added_ids):,}ê°œ ì¶”ê°€ë¨.\")\n",
    "\n",
    "            # ì¹´ìš´í„°ëŠ” ì´ë²ˆ ì‹¤í–‰ì—ì„œ *ìµœì¢…ì ìœ¼ë¡œ ë‚¨ì€* í–‰ì˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸\n",
    "            # ë˜ëŠ” ì „ì²´ ëˆ„ì  ì¹´ìš´íŠ¸ë¡œ ê´€ë¦¬í•  ìˆ˜ë„ ìˆìŒ (ì—¬ê¸°ì„  ìµœì¢… ì¹´ìš´íŠ¸ ì‚¬ìš©)\n",
    "            checkpoint_mgr.update_counter(\"reviews_processed\", final_count)\n",
    "            checkpoint_mgr.update_source_file(\"review_file\", csv_path)\n",
    "            # ìµœì¢… ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ì™„ë£Œë¡œ í‘œì‹œ\n",
    "            checkpoint_mgr.update_stage(stage_name, final_count > 0)\n",
    "            print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {stage_name}={'True' if final_count > 0 else 'False'}, count={final_count}\")\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“Š ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬ ìš”ì•½:\")\n",
    "        print(f\"âœ… ìµœì¢… í–‰ ìˆ˜: {final_count:,}\")\n",
    "        print(f\"âœ… ë¦¬ë·° ì»¬ëŸ¼: '{review_column}'\")\n",
    "        print(f\"âœ… ID ì»¬ëŸ¼: '{id_column if id_column else 'ì—†ìŒ'}'\")\n",
    "        print(f\"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        return processed_df, review_column, id_column\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ë¦¬ë·° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        print(\"=\"*70)\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "# enrich_review_data_fixed (ë³‘ë ¬ ì²˜ë¦¬ ì ìš©)\n",
    "def enrich_review_data_fixed(review_df, metadata_dict, meta_id_column, review_id_column,\n",
    "                             parallel_mgr=None, checkpoint_mgr=None, progress_bar=tqdm_notebook):\n",
    "    \"\"\"ë¦¬ë·° ë°ì´í„°ì— ê²Œì„ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ë³‘ë ¬ ì²˜ë¦¬ ë° ì²´í¬í¬ì¸íŠ¸ ì‹œë„)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ”„ 3ë‹¨ê³„: ë°ì´í„° í†µí•© (Enrichment)\")\n",
    "    print(\"=\"*70)\n",
    "    stage_name = \"data_enriched\"\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ---\n",
    "    if checkpoint_mgr and checkpoint_mgr.is_stage_completed(stage_name):\n",
    "         enriched_count = checkpoint_mgr.state[\"counters\"][\"enriched_count\"]\n",
    "         print(f\"âœ… ë°ì´í„° í†µí•© ë‹¨ê³„ëŠ” ì´ì „ì— ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (ì²˜ë¦¬ëœ í–‰: {enriched_count:,}).\")\n",
    "         # ì—¬ê¸°ì„œ ê±´ë„ˆë›°ë ¤ë©´ ì´ì „ ê²°ê³¼ ë¡œë“œ í•„ìš”. ì¼ë‹¨ ì§„í–‰.\n",
    "         print(\"   (ì²´í¬í¬ì¸íŠ¸ í™•ì¸. ë°ì´í„° í†µí•©ì„ ë‹¤ì‹œ ì§„í–‰í•©ë‹ˆë‹¤)\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if metadata_dict is None or review_df is None or review_df.empty:\n",
    "        print(\"âŒ ë©”íƒ€ë°ì´í„° ë˜ëŠ” ë¦¬ë·° ë°ì´í„°ê°€ ì—†ì–´ í†µí•© ë¶ˆê°€.\")\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        return review_df if review_df is not None else pd.DataFrame()\n",
    "\n",
    "    if not review_id_column or review_id_column not in review_df.columns:\n",
    "        print(f\"âš ï¸ ë¦¬ë·° ID ì»¬ëŸ¼ '{review_id_column}' ì—†ì–´ ë©”íƒ€ë°ì´í„° í†µí•© ë¶ˆê°€.\")\n",
    "        # ID ì—†ìœ¼ë©´ í†µí•© ë¶ˆê°€, ì›ë³¸ ë°˜í™˜\n",
    "        # ì²´í¬í¬ì¸íŠ¸ëŠ” ì™„ë£Œë˜ì§€ ì•ŠìŒ\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        return review_df\n",
    "\n",
    "    # ì‚¬ìš©í•  í•„ë“œ\n",
    "    fields_to_enrich = IMPORTANT_META_FIELDS\n",
    "    print(f\"   - í†µí•©í•  ë©”íƒ€ í•„ë“œ: {fields_to_enrich}\")\n",
    "\n",
    "    # ë°ì´í„° í†µí•©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ (ë‹¨ì¼ í–‰ ì²˜ë¦¬)\n",
    "    def enrich_single_row(row_tuple):\n",
    "        index, row_series = row_tuple # ì…ë ¥ì€ (ì¸ë±ìŠ¤, ì‹œë¦¬ì¦ˆ) í˜•íƒœ\n",
    "        enriched_row = row_series.to_dict() # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "        game_id = str(enriched_row.get(review_id_column, '')) # ID ì¶”ì¶œ ë° ë¬¸ìì—´ ë³€í™˜\n",
    "\n",
    "        # game_ í•„ë“œ ì´ˆê¸°í™”\n",
    "        for field in fields_to_enrich:\n",
    "            enriched_row[f'game_{field}'] = None\n",
    "\n",
    "        # ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ë° í•„ë“œ ë³µì‚¬\n",
    "        if game_id in metadata_dict:\n",
    "            game_meta = metadata_dict[game_id]\n",
    "            for field in fields_to_enrich:\n",
    "                if field in game_meta and pd.notna(game_meta[field]):\n",
    "                    enriched_row[f'game_{field}'] = game_meta[field]\n",
    "        # else: # ë§¤ì¹­ ì•ˆë˜ë©´ None ìœ ì§€ë¨\n",
    "\n",
    "        return enriched_row # ì²˜ë¦¬ëœ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜\n",
    "\n",
    "\n",
    "    # ì²˜ë¦¬í•  ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ (ì¸ë±ìŠ¤ í¬í•¨ íŠœí”Œ)\n",
    "    items_to_process = list(review_df.iterrows())\n",
    "\n",
    "    enriched_results = None\n",
    "    if parallel_mgr:\n",
    "        print(f\"\\nğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ê¸°ë¡œ ë©”íƒ€ë°ì´í„° í†µí•© ì‹œì‘ ({len(items_to_process):,}ê°œ í–‰)...\")\n",
    "        # process_batch í˜¸ì¶œ\n",
    "        enriched_results = parallel_mgr.process_batch(\n",
    "            enrich_single_row,\n",
    "            items_to_process,\n",
    "            desc=\"ë©”íƒ€ë°ì´í„° í†µí•© (ë³‘ë ¬)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\nğŸ”„ ì§ë ¬ ì²˜ë¦¬ë¡œ ë©”íƒ€ë°ì´í„° í†µí•© ì‹œì‘ ({len(items_to_process):,}ê°œ í–‰)...\")\n",
    "        enriched_results = []\n",
    "        for item in progress_bar(items_to_process, desc=\"ë©”íƒ€ë°ì´í„° í†µí•© (ì§ë ¬)\"):\n",
    "            try:\n",
    "                 enriched_results.append(enrich_single_row(item))\n",
    "            except Exception as e:\n",
    "                 print(f\"âš ï¸ í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "                 enriched_results.append(None) # ì˜¤ë¥˜ ì‹œ None ì¶”ê°€\n",
    "\n",
    "\n",
    "    # ê²°ê³¼ ì²˜ë¦¬ (ì˜¤ë¥˜ê°€ ë°œìƒí•œ í•­ëª©ì€ Noneì¼ ìˆ˜ ìˆìŒ)\n",
    "    valid_results = [res for res in enriched_results if res is not None]\n",
    "\n",
    "    if not valid_results:\n",
    "        print(\"âŒ ë°ì´í„° í†µí•© í›„ ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        return pd.DataFrame() # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜\n",
    "\n",
    "    enriched_df = pd.DataFrame(valid_results)\n",
    "    final_count = len(enriched_df)\n",
    "\n",
    "    # ë§¤ì¹­ë¥  ê³„ì‚° (ê°„ë‹¨íˆ game_Title í•„ë“œê°€ ì±„ì›Œì§„ ê²½ìš°ë¡œ ì¶”ì •)\n",
    "    matched_count = enriched_df[f'game_{fields_to_enrich[1]}'].notna().sum() # Title ê¸°ì¤€\n",
    "    match_pct = matched_count / final_count * 100 if final_count > 0 else 0\n",
    "    print(f\"\\nâœ… ë©”íƒ€ë°ì´í„° í†µí•© ì™„ë£Œ: {final_count:,}ê°œ í–‰ ì²˜ë¦¬ë¨\")\n",
    "    print(f\"   - ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ì¶”ì •: {matched_count:,} ({match_pct:.1f}%)\")\n",
    "\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ ---\n",
    "    if checkpoint_mgr:\n",
    "        checkpoint_mgr.update_counter(\"enriched_count\", final_count)\n",
    "        checkpoint_mgr.update_stage(stage_name, final_count > 0)\n",
    "        print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {stage_name}={'True' if final_count > 0 else 'False'}, count={final_count}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "    print(\"=\"*70)\n",
    "    return enriched_df\n",
    "\n",
    "\n",
    "# create_documents (ë³‘ë ¬ ì²˜ë¦¬ ì ìš©)\n",
    "def create_documents(df, review_column, enrich_text=True,\n",
    "                     parallel_mgr=None, checkpoint_mgr=None, progress_bar=tqdm_notebook):\n",
    "    \"\"\"ë¦¬ë·°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ Document ê°ì²´ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ğŸ“„ 4ë‹¨ê³„: Document ê°ì²´ ìƒì„± (enrich={enrich_text})\")\n",
    "    print(\"=\"*70)\n",
    "    stage_name = \"documents_created\"\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ---\n",
    "    if checkpoint_mgr and checkpoint_mgr.is_stage_completed(stage_name):\n",
    "        doc_count = checkpoint_mgr.state[\"counters\"][\"documents_created\"]\n",
    "        print(f\"âœ… Document ìƒì„± ë‹¨ê³„ëŠ” ì´ì „ì— ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (ìƒì„±ëœ ë¬¸ì„œ: {doc_count:,}).\")\n",
    "        # ì—¬ê¸°ì„œ ê±´ë„ˆë›°ë ¤ë©´ ì´ì „ ê²°ê³¼ ë¡œë“œ í•„ìš”. ì¼ë‹¨ ì§„í–‰.\n",
    "        print(\"   (ì²´í¬í¬ì¸íŠ¸ í™•ì¸. Document ìƒì„±ì„ ë‹¤ì‹œ ì§„í–‰í•©ë‹ˆë‹¤)\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(\"âŒ ì…ë ¥ ë°ì´í„°ê°€ ì—†ì–´ Document ìƒì„± ë¶ˆê°€.\")\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        return []\n",
    "\n",
    "    # í•„ë“œ ì´ë¦„ ë§¤í•‘ (ì´ì „ ì½”ë“œì™€ ë™ì¼)\n",
    "    field_display_names = {\n",
    "        'Game_Id': 'ê²Œì„ ID', 'Title': 'ê²Œì„', 'Name': 'ê²Œì„',\n",
    "        'Description': 'ì„¤ëª…', 'description_detail': 'ìƒì„¸ ì„¤ëª…',\n",
    "        'AvgRating': 'í‰ì ', 'yearpublished': 'ì¶œì‹œ ì—°ë„',\n",
    "        'minplayers': 'ìµœì†Œ í”Œë ˆì´ì–´', 'maxplayers': 'ìµœëŒ€ í”Œë ˆì´ì–´',\n",
    "        'playingtime': 'í”Œë ˆì´ ì‹œê°„', 'minage': 'ìµœì†Œ ì—°ë ¹',\n",
    "        'category_bert': 'BERT ì¹´í…Œê³ ë¦¬', 'CategoryType': 'ì¹´í…Œê³ ë¦¬ íƒ€ì…',\n",
    "        'boardgamecategory': 'ì¥ë¥´', 'boardgamemechanic': 'ë©”ì»¤ë‹ˆì¦˜',\n",
    "        'averageweight': 'ë³µì¡ë„', 'complexity': 'ë³µì¡ë„', 'weight': 'ë³µì¡ë„'\n",
    "    }\n",
    "\n",
    "    # Document ìƒì„±ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ (ë‹¨ì¼ í–‰ ì²˜ë¦¬)\n",
    "    def create_single_document(row_tuple):\n",
    "        index, row_series = row_tuple\n",
    "        row_dict = row_series.to_dict()\n",
    "\n",
    "        # 1. ë©”íƒ€ë°ì´í„° êµ¬ì„±\n",
    "        metadata = {}\n",
    "        for col, value in row_dict.items():\n",
    "            if col != review_column and pd.notna(value):\n",
    "                # íƒ€ì… ë³€í™˜ ì‹œë„\n",
    "                if isinstance(value, (np.int64, np.int32)): metadata[col] = int(value)\n",
    "                elif isinstance(value, (np.float64, np.float32)): metadata[col] = float(value)\n",
    "                elif isinstance(value, np.bool_): metadata[col] = bool(value)\n",
    "                else: metadata[col] = str(value) # ë‚˜ë¨¸ì§€ëŠ” ë¬¸ìì—´\n",
    "\n",
    "        # 2. page_content êµ¬ì„±\n",
    "        page_content = \"\"\n",
    "        review_text = str(row_dict.get(review_column, ''))\n",
    "\n",
    "        if not review_text.strip():\n",
    "            return None # ë¦¬ë·° ì—†ìœ¼ë©´ None ë°˜í™˜\n",
    "\n",
    "        if enrich_text:\n",
    "            combined_parts = []\n",
    "            # IMPORTANT_META_FIELDS ìˆœíšŒ\n",
    "            for field_name in IMPORTANT_META_FIELDS:\n",
    "                col_name_in_df = f\"game_{field_name}\"\n",
    "                if col_name_in_df in row_dict and pd.notna(row_dict[col_name_in_df]):\n",
    "                    value = row_dict[col_name_in_df]\n",
    "                    display_name = field_display_names.get(field_name, field_name)\n",
    "                    formatted_value = \"\"\n",
    "                    # (ê°’ í¬ë§·íŒ… ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ì ìš©)\n",
    "                    try:\n",
    "                        if field_name in ['minplayers', 'maxplayers', 'playingtime', 'minage']:\n",
    "                            formatted_value = f\"{int(float(value))}\"\n",
    "                            if field_name in ['minplayers', 'maxplayers']: formatted_value += \"ëª…\"\n",
    "                            if field_name == 'playingtime': formatted_value += \"ë¶„\"\n",
    "                            if field_name == 'minage': formatted_value += \"ì„¸ ì´ìƒ\"\n",
    "                        elif field_name in ['averageweight', 'complexity', 'weight']:\n",
    "                            formatted_value = f\"{float(value):.2f}/5\"\n",
    "                        elif field_name == 'AvgRating':\n",
    "                            formatted_value = f\"{float(value):.2f}ì \"\n",
    "                        elif field_name in ['Description', 'description_detail'] and len(str(value)) > 200:\n",
    "                             formatted_value = f\"{str(value)[:200]}...\" # ì„¤ëª… ê¸¸ì´ëŠ” ì¡°ê¸ˆ ì¤„ì„\n",
    "                        elif isinstance(value, str) and value.strip().startswith('[') and value.strip().endswith(']'):\n",
    "                             try:\n",
    "                                 parsed_list = ast.literal_eval(value); formatted_value = \", \".join(map(str, parsed_list)) if isinstance(parsed_list, list) else str(value)\n",
    "                             except: formatted_value = value.strip(\"[]\").replace(\"'\", \"\").replace('\"', '')\n",
    "                        elif field_name != 'Game_Id': # Game_IdëŠ” ë³´í†µ ë‚´ìš©ì— í¬í•¨ ì•ˆ í•¨\n",
    "                             formatted_value = str(value)\n",
    "\n",
    "                        # ë‚´ìš© ì¶”ê°€ (Titleì€ ë ˆì´ë¸” ì—†ì´, ì„¤ëª…/ID ì œì™¸)\n",
    "                        if formatted_value and field_name not in ['Game_Id', 'Description', 'description_detail']:\n",
    "                            if field_name == 'Title': combined_parts.insert(0, f\"{display_name}: {formatted_value}\")\n",
    "                            else: combined_parts.append(f\"{display_name}: {formatted_value}\")\n",
    "\n",
    "                    except Exception: pass # í¬ë§·íŒ… ì˜¤ë¥˜ ì‹œ í•´ë‹¹ í•„ë“œ ê±´ë„ˆëœ€\n",
    "\n",
    "            # ì„¤ëª… í•„ë“œ ì¶”ê°€\n",
    "            desc_text = \"\"\n",
    "            desc_col = next((c for c in [f'game_{f}' for f in ['Description', 'description', 'description_detail']] if c in row_dict and pd.notna(row_dict[c])), None)\n",
    "            if desc_col:\n",
    "                desc_val = str(row_dict[desc_col])\n",
    "                desc_text = f\"ì„¤ëª…: {desc_val[:200]}...\" if len(desc_val) > 200 else f\"ì„¤ëª…: {desc_val}\"\n",
    "\n",
    "            # ìµœì¢… page_content ê²°í•©\n",
    "            content_elements = [item for item in [ \"\\n\".join(combined_parts), desc_text, f\"ë¦¬ë·°: {review_text}\"] if item.strip()]\n",
    "            page_content = \"\\n\\n\".join(content_elements)\n",
    "\n",
    "        else: # enrich_text=False ì´ë©´ ë¦¬ë·°ë§Œ ì‚¬ìš©\n",
    "            page_content = review_text\n",
    "\n",
    "        # Document ê°ì²´ ìƒì„±\n",
    "        if page_content:\n",
    "            return Document(page_content=page_content, metadata=metadata)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # ì²˜ë¦¬í•  ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸\n",
    "    items_to_process = list(df.iterrows())\n",
    "    document_results = None\n",
    "\n",
    "    if parallel_mgr:\n",
    "         print(f\"\\nğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ê¸°ë¡œ Document ìƒì„± ì‹œì‘ ({len(items_to_process):,}ê°œ í–‰)...\")\n",
    "         document_results = parallel_mgr.process_batch(\n",
    "             create_single_document,\n",
    "             items_to_process,\n",
    "             desc=\"Document ìƒì„± (ë³‘ë ¬)\"\n",
    "         )\n",
    "    else:\n",
    "         print(f\"\\nğŸ”„ ì§ë ¬ ì²˜ë¦¬ë¡œ Document ìƒì„± ì‹œì‘ ({len(items_to_process):,}ê°œ í–‰)...\")\n",
    "         document_results = []\n",
    "         for item in progress_bar(items_to_process, desc=\"Document ìƒì„± (ì§ë ¬)\"):\n",
    "             try:\n",
    "                  doc = create_single_document(item)\n",
    "                  if doc: document_results.append(doc)\n",
    "                  # else: # ë¦¬ë·° ì—†ê±°ë‚˜ ë‚´ìš© ìƒì„± ì‹¤íŒ¨ ì‹œ ê±´ë„ˆëœ€\n",
    "             except Exception as e:\n",
    "                  print(f\"âš ï¸ í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "                  # ì˜¤ë¥˜ ë°œìƒ í–‰ì€ ê±´ë„ˆëœ€\n",
    "\n",
    "    # ê²°ê³¼ í•„í„°ë§ (None ì œê±°)\n",
    "    final_documents = [doc for doc in document_results if doc is not None]\n",
    "    final_count = len(final_documents)\n",
    "\n",
    "    print(f\"\\nâœ… Document ìƒì„± ì™„ë£Œ: {final_count:,}ê°œ ìƒì„±ë¨\")\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ ---\n",
    "    if checkpoint_mgr:\n",
    "        checkpoint_mgr.update_counter(\"documents_created\", final_count)\n",
    "        checkpoint_mgr.update_stage(stage_name, final_count > 0)\n",
    "        print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {stage_name}={'True' if final_count > 0 else 'False'}, count={final_count}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "    print(\"=\"*70)\n",
    "    return final_documents\n",
    "\n",
    "\n",
    "# split_documents (ë³€ê²½ ì—†ìŒ - ë‚´ë¶€ ë¡œì§ì€ ìœ ì§€)\n",
    "def split_documents(documents, max_chunk_size=MAX_CHUNK_SIZE, checkpoint_mgr=None, progress_bar=tqdm_notebook):\n",
    "    \"\"\"ê¸´ ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ‚ï¸ 5ë‹¨ê³„: ë¬¸ì„œ ë¶„í• \")\n",
    "    print(\"=\"*70)\n",
    "    stage_name = \"documents_split\"\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ---\n",
    "    if checkpoint_mgr and checkpoint_mgr.is_stage_completed(stage_name):\n",
    "         split_count = checkpoint_mgr.state[\"counters\"][\"documents_split\"]\n",
    "         print(f\"âœ… ë¬¸ì„œ ë¶„í•  ë‹¨ê³„ëŠ” ì´ì „ì— ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (ë¶„í• ëœ ë¬¸ì„œ: {split_count:,}).\")\n",
    "         # ì—¬ê¸°ì„œ ê±´ë„ˆë›°ë ¤ë©´ ì´ì „ ê²°ê³¼ ë¡œë“œ í•„ìš”. ì¼ë‹¨ ì§„í–‰.\n",
    "         print(\"   (ì²´í¬í¬ì¸íŠ¸ í™•ì¸. ë¬¸ì„œ ë¶„í• ì„ ë‹¤ì‹œ ì§„í–‰í•©ë‹ˆë‹¤)\")\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not documents:\n",
    "        print(\"âŒ ë¶„í• í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        return []\n",
    "\n",
    "    # (ë¬¸ì„œ ê¸¸ì´ í†µê³„ ê³„ì‚° ë¡œì§ ìœ ì§€)\n",
    "    doc_lengths = [len(doc.page_content) for doc in documents if hasattr(doc, 'page_content')]\n",
    "    if not doc_lengths: print(\"âŒ ìœ íš¨í•œ ë¬¸ì„œ ë‚´ìš©ì´ ì—†ì–´ ë¶„í•  ë¶ˆê°€.\"); return documents\n",
    "    max_len = max(doc_lengths)\n",
    "    print(f\"   - ìµœëŒ€ ë¬¸ì„œ ê¸¸ì´: {max_len:,}ì\")\n",
    "\n",
    "    if max_len <= max_chunk_size:\n",
    "        print(f\"âœ… ëª¨ë“  ë¬¸ì„œê°€ ì²­í¬ í¬ê¸°({max_chunk_size}) ì´í•˜ì´ë¯€ë¡œ ë¶„í•  ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        if checkpoint_mgr:\n",
    "             # ë¶„í• ì´ í•„ìš” ì—†ì—ˆìœ¼ë¯€ë¡œ ì™„ë£Œë¡œ ê°„ì£¼, ì¹´ìš´íŠ¸ëŠ” ì›ë³¸ ë¬¸ì„œ ìˆ˜\n",
    "             checkpoint_mgr.update_counter(\"documents_split\", len(documents))\n",
    "             checkpoint_mgr.update_stage(stage_name, True)\n",
    "        return documents\n",
    "\n",
    "    print(f\"\\nğŸ”ª ë¬¸ì„œë¥¼ {max_chunk_size}ì ë‹¨ìœ„ë¡œ ë¶„í•  ì¤‘...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=int(max_chunk_size * 0.1),\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \", \", \" \", \"\"],\n",
    "        keep_separator=False\n",
    "    )\n",
    "\n",
    "    split_docs = []\n",
    "    split_errors = 0\n",
    "    # ë¶„í• ì€ ë³‘ë ¬í™” íš¨ê³¼ê°€ í¬ì§€ ì•Šì„ ìˆ˜ ìˆìŒ (LangChain ë‚´ë¶€ ìµœì í™” ê°€ëŠ¥ì„±)\n",
    "    # ì—¬ê¸°ì„œëŠ” ì§ë ¬ ì²˜ë¦¬ ìœ ì§€, í•„ìš” ì‹œ ë³‘ë ¬í™” ê³ ë ¤\n",
    "    try:\n",
    "        split_docs = text_splitter.split_documents(documents)\n",
    "    except Exception as e:\n",
    "         print(f\"âŒ ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "         split_errors += 1\n",
    "         # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°˜í™˜ ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ ê²°ì • í•„ìš”\n",
    "         if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "         return documents # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°˜í™˜\n",
    "\n",
    "    final_count = len(split_docs)\n",
    "    print(f\"\\nğŸ“Š ë¶„í•  ê²°ê³¼: {len(documents):,}ê°œ -> {final_count:,}ê°œ ë¬¸ì„œ\")\n",
    "    if split_errors > 0: print(f\"   âš ï¸ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ ê±´ìˆ˜: {split_errors:,}\")\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ ---\n",
    "    if checkpoint_mgr:\n",
    "        checkpoint_mgr.update_counter(\"documents_split\", final_count)\n",
    "        checkpoint_mgr.update_stage(stage_name, final_count > 0)\n",
    "        print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {stage_name}={'True' if final_count > 0 else 'False'}, count={final_count}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "\n",
    "# setup_vectorstore (ë³€ê²½ ì—†ìŒ - ë‚´ë¶€ ë¡œì§ì€ ìœ ì§€)\n",
    "def setup_vectorstore(model_name=MODEL_NAME, persist_dir=CHROMA_PERSIST_DIR, device=\"cpu\", checkpoint_mgr=None):\n",
    "    \"\"\"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ë˜ëŠ” ë¡œë“œ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ—ï¸ 6ë‹¨ê³„: ë²¡í„° ì €ì¥ì†Œ ì„¤ì •/ë¡œë“œ\")\n",
    "    print(\"=\"*70)\n",
    "    stage_name = \"vectorstore_setup\" # setup ë‹¨ê³„ ìì²´ëŠ” ìƒíƒœ ì €ì¥ì´ ëœ ì¤‘ìš”í•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"   - ëª¨ë¸: {model_name}\")\n",
    "    print(f\"   - ì €ì¥ ê²½ë¡œ: {persist_dir}\")\n",
    "    print(f\"   - ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
    "\n",
    "    # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "    embeddings = None\n",
    "    try:\n",
    "        print(\"\\nğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': device},\n",
    "            encode_kwargs={'normalize_embeddings': True, 'batch_size': 128} # ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°\n",
    "        )\n",
    "        print(f\"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({model_name})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        return None # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì§„í–‰ ë¶ˆê°€\n",
    "\n",
    "    # 2. ChromaDB ì„¤ì • ë˜ëŠ” ë¡œë“œ\n",
    "    vectorstore = None\n",
    "    try:\n",
    "        print(\"\\nğŸ’¾ ChromaDB ì„¤ì •/ë¡œë“œ ì¤‘...\")\n",
    "        if os.path.exists(persist_dir) and os.listdir(persist_dir):\n",
    "            print(f\"   â„¹ï¸ ê¸°ì¡´ DB ë°œê²¬: {persist_dir}. ë¡œë“œ ì‹œë„...\")\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=persist_dir,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "            print(f\"   âœ… ê¸°ì¡´ ChromaDB ë¡œë“œ ì™„ë£Œ.\")\n",
    "            try:\n",
    "                 count = vectorstore._collection.count()\n",
    "                 print(f\"   ğŸ“Š ë¡œë“œëœ DB ë¬¸ì„œ ìˆ˜: {count:,}\")\n",
    "            except Exception as e_count:\n",
    "                 print(f\"   âš ï¸ ë¡œë“œëœ DB ë¬¸ì„œ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e_count}\")\n",
    "        else:\n",
    "            print(f\"   â„¹ï¸ ìƒˆ ChromaDB ìƒì„±: {persist_dir}\")\n",
    "            os.makedirs(persist_dir, exist_ok=True)\n",
    "            # ë¬¸ì„œëŠ” add_documents ë‹¨ê³„ì—ì„œ ì¶”ê°€í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¹ˆ DB ìƒì„±\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=persist_dir,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "            print(f\"   âœ… ìƒˆ ChromaDB ì¸ìŠ¤í„´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ChromaDB ì„¤ì •/ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        return None\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ ---\n",
    "    # setup ë‹¨ê³„ëŠ” ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ë§Œ ê¸°ë¡í•´ë„ ì¶©ë¶„í•  ìˆ˜ ìˆìŒ\n",
    "    if checkpoint_mgr:\n",
    "         checkpoint_mgr.update_stage(stage_name, vectorstore is not None)\n",
    "         print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {stage_name}={'True' if vectorstore else 'False'}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nâ±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# add_documents_to_vectorstore (ì²´í¬í¬ì¸íŠ¸ ID í™•ì¸ ë° ë°°ì¹˜ ì¬ê°œ ë¡œì§ ì¶”ê°€)\n",
    "def add_documents_to_vectorstore(documents, vectorstore, batch_size=BATCH_SIZE, checkpoint_mgr=None, progress_bar=tqdm_notebook):\n",
    "    \"\"\"ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬, ID ì¤‘ë³µ ë°©ì§€, ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“¥ 7ë‹¨ê³„: ë²¡í„° ì €ì¥ì†Œì— ë¬¸ì„œ ì¶”ê°€\")\n",
    "    print(\"=\"*70)\n",
    "    stage_name = \"documents_added\" # ì´ ë‹¨ê³„ì˜ ì™„ë£Œ ì—¬ë¶€ê°€ ì¤‘ìš”\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not documents:\n",
    "        print(\"âœ… ì¶”ê°€í•  ìƒˆ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        # ë¬¸ì„œê°€ ì—†ì–´ë„ ë‹¨ê³„ëŠ” 'ì™„ë£Œ'ë¡œ ë³¼ ìˆ˜ ìˆìŒ (í•  ì¼ì´ ì—†ìœ¼ë¯€ë¡œ)\n",
    "        if checkpoint_mgr and not checkpoint_mgr.is_stage_completed(stage_name):\n",
    "              checkpoint_mgr.update_stage(stage_name, True)\n",
    "              print(\"   (ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: ì¶”ê°€í•  ë¬¸ì„œ ì—†ìŒ, ì™„ë£Œë¡œ ê°„ì£¼)\")\n",
    "        return vectorstore\n",
    "\n",
    "    if not vectorstore:\n",
    "        print(\"âŒ ë²¡í„° ì €ì¥ì†Œê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë¬¸ì„œ ì¶”ê°€ ë¶ˆê°€.\")\n",
    "        if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, False)\n",
    "        return None\n",
    "\n",
    "    total_docs_to_process = len(documents)\n",
    "    print(f\"   - ì¶”ê°€ ì‹œë„ ë¬¸ì„œ ìˆ˜: {total_docs_to_process:,}\")\n",
    "    print(f\"   - ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "\n",
    "    # --- ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¬ê°œ ë¡œì§ ---\n",
    "    start_index = 0\n",
    "    previously_added_ids = set()\n",
    "    if checkpoint_mgr:\n",
    "        # ì´ì „ì— ë§ˆì§€ë§‰ìœ¼ë¡œ ì„±ê³µí•œ ë°°ì¹˜ ì¸ë±ìŠ¤ í™•ì¸\n",
    "        last_added_batch_idx = checkpoint_mgr.state[\"counters\"].get(\"last_added_batch_index\", -1)\n",
    "        if last_added_batch_idx >= 0:\n",
    "             start_index = (last_added_batch_idx + 1) * batch_size\n",
    "             print(f\"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: ë§ˆì§€ë§‰ ì„±ê³µ ë°°ì¹˜ ì¸ë±ìŠ¤ {last_added_batch_idx}.\")\n",
    "             if start_index >= total_docs_to_process:\n",
    "                 print(\"   âœ… ëª¨ë“  ë¬¸ì„œê°€ ì´ë¯¸ ì´ì „ ì‹¤í–‰ì—ì„œ ì¶”ê°€ëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\")\n",
    "                 if not checkpoint_mgr.is_stage_completed(stage_name):\n",
    "                      checkpoint_mgr.update_stage(stage_name, True) # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "                 return vectorstore\n",
    "             else:\n",
    "                  print(f\"   -> ì¸ë±ìŠ¤ {start_index}ë¶€í„° ë¬¸ì„œ ì¶”ê°€ë¥¼ ì¬ê°œí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "        # ì´ë¯¸ ì¶”ê°€ëœ ë¬¸ì„œ ID ë¡œë“œ (ì¤‘ë³µ ì¶”ê°€ ë°©ì§€ìš©)\n",
    "        previously_added_ids = checkpoint_mgr.state[\"processed_ids\"].get(\"document_ids\", set())\n",
    "        if previously_added_ids:\n",
    "             print(f\"   â„¹ï¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œëœ ì´ì „ ì¶”ê°€ ë¬¸ì„œ ID: {len(previously_added_ids):,}ê°œ (ì¤‘ë³µ ë°©ì§€ìš©)\")\n",
    "\n",
    "\n",
    "    # ë¬¸ì„œ ID ìƒì„± ë° í•„í„°ë§ (ì¬ê°œ ì¸ë±ìŠ¤ ë° ì¤‘ë³µ ID ê³ ë ¤)\n",
    "    docs_to_add_this_run = []\n",
    "    ids_to_add_this_run = []\n",
    "    processed_for_id_gen = 0\n",
    "    skipped_already_added = 0\n",
    "    unique_generated_ids = set(previously_added_ids) # ê¸°ì¡´ ID í¬í•¨í•˜ì—¬ ì‹œì‘\n",
    "\n",
    "    # ID ìƒì„±ê¸° (ì¶©ëŒ ë°©ì§€ í¬í•¨)\n",
    "    import hashlib\n",
    "    import uuid\n",
    "    def generate_unique_doc_id(doc, idx, existing_ids):\n",
    "        content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()[:8]\n",
    "        base_id = f\"doc_{idx}_{content_hash}\" # ì¸ë±ìŠ¤ì™€ ë‚´ìš© í•´ì‹œ ê¸°ë°˜\n",
    "        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹ë³„ì ì‚¬ìš© ì‹œë„ (ì„ íƒì )\n",
    "        # meta_id = doc.metadata.get('game_Game_Id', doc.metadata.get('id'))\n",
    "        # if meta_id: base_id = f\"{meta_id}_{content_hash}\"\n",
    "\n",
    "        final_id = base_id\n",
    "        counter = 0\n",
    "        while final_id in existing_ids:\n",
    "            counter += 1\n",
    "            final_id = f\"{base_id}_dup{counter}\"\n",
    "        return final_id\n",
    "\n",
    "    print(\"   ID ìƒì„± ë° ì¤‘ë³µ í™•ì¸ ì¤‘...\")\n",
    "    # ì¬ê°œ ì§€ì ë¶€í„° ID ìƒì„± ë° í•„í„°ë§\n",
    "    for idx, doc in enumerate(documents[start_index:], start=start_index):\n",
    "        processed_for_id_gen += 1\n",
    "        generated_id = generate_unique_doc_id(doc, idx, unique_generated_ids)\n",
    "\n",
    "        # previously_added_ids ì— ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ DBì— ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ)\n",
    "        if generated_id in previously_added_ids:\n",
    "            skipped_already_added += 1\n",
    "            continue\n",
    "\n",
    "        docs_to_add_this_run.append(doc)\n",
    "        ids_to_add_this_run.append(generated_id)\n",
    "        unique_generated_ids.add(generated_id) # ìƒˆë¡œ ìƒì„±ëœ IDë„ ì§‘í•©ì— ì¶”ê°€\n",
    "\n",
    "    print(f\"   ID ìƒì„± ì™„ë£Œ: {processed_for_id_gen}ê°œ í™•ì¸, {skipped_already_added}ê°œ ê±´ë„ˆëœ€.\")\n",
    "    print(f\"   ì´ë²ˆ ì‹¤í–‰ì—ì„œ DBì— ì¶”ê°€í•  ë¬¸ì„œ ìˆ˜: {len(docs_to_add_this_run):,}\")\n",
    "\n",
    "    if not docs_to_add_this_run:\n",
    "        print(\"âœ… ì´ë²ˆ ì‹¤í–‰ì—ì„œ ì¶”ê°€í•  ìƒˆ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        if checkpoint_mgr and not checkpoint_mgr.is_stage_completed(stage_name):\n",
    "             checkpoint_mgr.update_stage(stage_name, True) # ì¶”ê°€í•  ê²ƒ ì—†ìœ¼ë©´ ì™„ë£Œ\n",
    "        return vectorstore\n",
    "\n",
    "    # ë°°ì¹˜ ì²˜ë¦¬\n",
    "    total_batches = (len(docs_to_add_this_run) + batch_size - 1) // batch_size\n",
    "    added_count_this_run = 0\n",
    "    current_batch_index_global = (start_index // batch_size) # ì „ì—­ ë°°ì¹˜ ì¸ë±ìŠ¤ (ì²´í¬í¬ì¸íŠ¸ìš©)\n",
    "\n",
    "    print(f\"\\nğŸ”„ {total_batches}ê°œ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë¬¸ì„œ ì¶”ê°€ ì‹œì‘ (ì „ì—­ ë°°ì¹˜ ì¸ë±ìŠ¤ {current_batch_index_global}ë¶€í„°)...\")\n",
    "\n",
    "    # DB ì¶”ê°€ ì „ í˜„ì¬ ë¬¸ì„œ ìˆ˜ í™•ì¸\n",
    "    try: initial_db_count = vectorstore._collection.count()\n",
    "    except: initial_db_count = -1 # í™•ì¸ ë¶ˆê°€ ì‹œ\n",
    "\n",
    "    # ë°°ì¹˜ ë£¨í”„\n",
    "    for i in range(0, len(docs_to_add_this_run), batch_size):\n",
    "        batch_docs = docs_to_add_this_run[i : i + batch_size]\n",
    "        batch_ids = ids_to_add_this_run[i : i + batch_size]\n",
    "        current_batch_index_local = i // batch_size # ì´ë²ˆ ì‹¤í–‰ ë‚´ ë°°ì¹˜ ì¸ë±ìŠ¤\n",
    "        current_batch_index_global = (start_index + i) // batch_size # ì „ì—­ ë°°ì¹˜ ì¸ë±ìŠ¤\n",
    "\n",
    "        progress_desc = f\"ë²¡í„° ì €ì¥ì†Œ ì¶”ê°€ (ë°°ì¹˜ {current_batch_index_local+1}/{total_batches}, ì „ì—­ {current_batch_index_global})\"\n",
    "\n",
    "        try:\n",
    "             # tqdm ì ìš©í•˜ì—¬ add_documents í˜¸ì¶œ\n",
    "             # ì°¸ê³ : add_documents ìì²´ê°€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, tqdmì´ ì¦‰ê° ë°˜ì‘ ì•ˆ í•  ìˆ˜ ìˆìŒ\n",
    "             #       í•˜ì§€ë§Œ ë°°ì¹˜ ë‹¨ìœ„ë¡œëŠ” í‘œì‹œë¨\n",
    "             vectorstore.add_documents(documents=batch_docs, ids=batch_ids)\n",
    "             added_count_this_run += len(batch_docs)\n",
    "\n",
    "             # --- ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ (ë°°ì¹˜ ì„±ê³µ ì‹œ) ---\n",
    "             if checkpoint_mgr:\n",
    "                 # ì„±ê³µí•œ ì „ì—­ ë°°ì¹˜ ì¸ë±ìŠ¤ ì €ì¥\n",
    "                 checkpoint_mgr.update_counter(\"last_added_batch_index\", current_batch_index_global)\n",
    "                 # ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ëœ ID ì €ì¥ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ì˜í•˜ë©° ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥)\n",
    "                 checkpoint_mgr.add_processed_ids(\"document_ids\", batch_ids)\n",
    "                 # ëˆ„ì  ì¶”ê°€ ì¹´ìš´í„° ì—…ë°ì´íŠ¸ (ì„ íƒì )\n",
    "                 # checkpoint_mgr.update_counter(\"documents_added\", checkpoint_mgr.state[\"counters\"][\"documents_added\"] + len(batch_docs))\n",
    "\n",
    "             # ì§„í–‰ë¥  í‘œì‹œ (tqdmê³¼ ë³„ê°œë¡œ)\n",
    "             if (current_batch_index_local + 1) % 10 == 0: # 10 ë°°ì¹˜ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥\n",
    "                  print(f\"   ì§„í–‰: {current_batch_index_local + 1}/{total_batches} ë°°ì¹˜ ì™„ë£Œ ({added_count_this_run:,}ê°œ ë¬¸ì„œ ì¶”ê°€ë¨)\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ ë°°ì¹˜ {current_batch_index_local + 1} (ì „ì—­ {current_batch_index_global}) ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë°°ì¹˜ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ë°°ì¹˜ ì§„í–‰\n",
    "            # ë˜ëŠ” íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨ ê²°ì •\n",
    "            print(\"   âš ï¸ í•´ë‹¹ ë°°ì¹˜ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ëŠ” í•˜ì§€ ì•ŠìŒ\n",
    "            continue # ë‹¤ìŒ ë°°ì¹˜ë¡œ\n",
    "\n",
    "\n",
    "    print(f\"\\nâœ… ë¬¸ì„œ ì¶”ê°€ ì‘ì—… ì™„ë£Œ (ì´ë²ˆ ì‹¤í–‰ì—ì„œ {added_count_this_run:,}ê°œ ì‹œë„).\")\n",
    "\n",
    "    # ìµœì¢… ì €ì¥ (persist)\n",
    "    try:\n",
    "        print(\"\\nğŸ’¾ ë³€ê²½ì‚¬í•­ ì €ì¥ ì¤‘...\")\n",
    "        vectorstore.persist()\n",
    "        print(\"âœ… ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e_persist:\n",
    "        print(f\"   âš ï¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_persist}\")\n",
    "\n",
    "    # ìµœì¢… ë¬¸ì„œ ìˆ˜ í™•ì¸\n",
    "    try:\n",
    "        final_db_count = vectorstore._collection.count()\n",
    "        print(f\"\\nğŸ“Š ìµœì¢… DB ë¬¸ì„œ ìˆ˜ (í™•ì¸): {final_db_count:,}\")\n",
    "        if initial_db_count != -1:\n",
    "            net_added = final_db_count - initial_db_count\n",
    "            print(f\"   - ìˆœ ì¦ê°€ ë¬¸ì„œ ìˆ˜: {net_added:,}\")\n",
    "    except Exception as e:\n",
    "        print(\"\\nâš ï¸ ìµœì¢… ë¬¸ì„œ ìˆ˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "\n",
    "    # --- ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ ---\n",
    "    if checkpoint_mgr:\n",
    "        final_added_count = checkpoint_mgr.get_processed_ids_count(\"document_ids\")\n",
    "        checkpoint_mgr.update_counter(\"documents_added\", final_added_count) # ì „ì²´ ëˆ„ì  ID ìˆ˜ë¡œ ì—…ë°ì´íŠ¸\n",
    "\n",
    "        # ëª¨ë“  ë¬¸ì„œê°€ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸ í›„ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "        # (start_index + len(docs_to_add_this_run)) ì´ ì›ë˜ ë¬¸ì„œ ìˆ˜ì™€ ê°™ìœ¼ë©´ ì™„ë£Œ\n",
    "        is_complete = (start_index + len(docs_to_add_this_run)) >= total_docs_to_process\n",
    "        if is_complete:\n",
    "             checkpoint_mgr.update_stage(stage_name, True)\n",
    "             print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {stage_name}=True (ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ)\")\n",
    "        else:\n",
    "             # ì•„ì§ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ë‚¨ìŒ (ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ì¤‘ë‹¨ëœ ê²½ìš°)\n",
    "             checkpoint_mgr.update_stage(stage_name, False)\n",
    "             print(f\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {stage_name}=False (ì•„ì§ ì²˜ë¦¬í•  ë¬¸ì„œ ë‚¨ìŒ)\")\n",
    "\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nâ±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# test_vectorstore (ë³€ê²½ ì—†ìŒ - ë‚´ë¶€ ë¡œì§ì€ ìœ ì§€)\n",
    "def test_vectorstore(vectorstore, checkpoint_mgr=None):\n",
    "    \"\"\"ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸ (E5 ì¿¼ë¦¬ í¬ë§·íŒ… ì ìš©)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70); print(\"ğŸ§ª 8ë‹¨ê³„: ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸\"); print(\"=\"*70)\n",
    "    stage_name=\"vectorstore_tested\"\n",
    "\n",
    "    if vectorstore is None: print(\"âŒ ì €ì¥ì†Œ ì—†ìŒ.\"); return None\n",
    "\n",
    "    try: doc_count = vectorstore._collection.count(); print(f\"ğŸ“Š ë¬¸ì„œ ìˆ˜: {doc_count:,}\")\n",
    "    except: doc_count = 0; print(\"âš ï¸ ë¬¸ì„œ ìˆ˜ í™•ì¸ ë¶ˆê°€\")\n",
    "    if doc_count == 0: print(\"   âš ï¸ ë¬¸ì„œ ì—†ìŒ.\"); return vectorstore\n",
    "\n",
    "    test_queries = [ \"ì•„ì´ë“¤ê³¼ í•¨ê»˜í•˜ëŠ” ê²Œì„\", \"ì „ëµì ì¸ ê²Œì„ ì¶”ì²œ\", \"ì§§ì€ í”Œë ˆì´ íƒ€ì„ì˜ ê²Œì„\", \"Catan\" ]\n",
    "    print(\"\\nğŸ” ê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ (ìƒìœ„ 3ê°œ ê²°ê³¼):\")\n",
    "\n",
    "    # --- <<< E5 ëª¨ë¸ í™•ì¸ ë° ì¿¼ë¦¬ í¬ë§·íŒ… ì ìš© >>> ---\n",
    "    is_e5_model = MODEL_NAME.startswith(\"intfloat/e5\") or MODEL_NAME.startswith(\"intfloat/multilingual-e5\")\n",
    "    if is_e5_model:\n",
    "        print(\"   â„¹ï¸ E5 ëª¨ë¸ ê°ì§€: ê²€ìƒ‰ ì¿¼ë¦¬ì— 'query: ' ì ‘ë‘ì‚¬ ì ìš©\")\n",
    "\n",
    "    for query in test_queries:\n",
    "        # --- <<< ì¿¼ë¦¬ í¬ë§·íŒ… ì ìš© >>> ---\n",
    "        search_query = format_query(query) if is_e5_model else query\n",
    "        print(f\"\\nğŸ“ ì¿¼ë¦¬: '{query}' {'(í¬ë§·íŒ… ì ìš©ë¨)' if is_e5_model else ''}\")\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            results = vectorstore.similarity_search_with_score(search_query, k=3) # í¬ë§·íŒ…ëœ ì¿¼ë¦¬ ì‚¬ìš©\n",
    "            search_time = time.time() - start_time\n",
    "            print(f\"â±ï¸ ê²€ìƒ‰ ì‹œê°„: {search_time*1000:.2f}ms\")\n",
    "            if results:\n",
    "                print(f\"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):\")\n",
    "                for i, (doc, score) in enumerate(results):\n",
    "                    title = doc.metadata.get('game_Title', doc.metadata.get('Title', '?'))\n",
    "                    game_id = doc.metadata.get('game_Game_Id', doc.metadata.get('Game_Id', '?'))\n",
    "                    # --- <<< Passage ì ‘ë‘ì‚¬ ì œê±° í›„ ì¶œë ¥ (ì„ íƒì ) >>> ---\n",
    "                    content_preview = doc.page_content\n",
    "                    if content_preview.startswith(\"passage: \"):\n",
    "                         content_preview = content_preview[len(\"passage: \"):]\n",
    "                    content_preview = content_preview.replace('\\n', ' ').strip()\n",
    "                    print(f\"  Rank {i+1} (Score: {score:.4f}) ID:{game_id} Title:{title}\")\n",
    "                    print(f\"    ë‚´ìš©: {content_preview[:150]}...\")\n",
    "            else: print(\"  âŒ ê²°ê³¼ ì—†ìŒ\")\n",
    "        except Exception as e: print(f\"âŒ ì¿¼ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    if checkpoint_mgr: checkpoint_mgr.update_stage(stage_name, True)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# --- [ìˆ˜ì •ëœ run_pipeline í•¨ìˆ˜] ---\n",
    "def run_pipeline(\n",
    "    resume=False,\n",
    "    reset_checkpoint=False,\n",
    "    use_parallel=True, # ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ í”Œë˜ê·¸\n",
    "    parallel_use_processes=False # ë³‘ë ¬ ì²˜ë¦¬ ë°©ì‹ (False: ìŠ¤ë ˆë“œ, True: í”„ë¡œì„¸ìŠ¤)\n",
    "):\n",
    "    \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì²´í¬í¬ì¸íŠ¸, ë³‘ë ¬ ì²˜ë¦¬ ì ìš©)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸš€ ë³´ë“œê²Œì„ ì¶”ì²œ ì‹œìŠ¤í…œ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ê°œì„  ë²„ì „)\")\n",
    "    print(f\"â±ï¸ ì‹œì‘ ì‹œê°„: {time.strftime('%H:%M:%S')}\")\n",
    "    print(f\"ğŸ”„ ì˜µì…˜: resume={resume}, reset_checkpoint={reset_checkpoint}, use_parallel={use_parallel}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    pipeline_start = time.time()\n",
    "    vectorstore = None\n",
    "\n",
    "    # 0. ì´ˆê¸° ì„¤ì • ë° ì§„ë‹¨\n",
    "    print(\"\\nğŸ“‹ 0ë‹¨ê³„: ì´ˆê¸° ì„¤ì • ë° ì§„ë‹¨\")\n",
    "    has_gpu = check_gpu_status()\n",
    "    device = \"cuda\" if has_gpu else \"cpu\"\n",
    "    print(f\"ğŸ–¥ï¸ ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
    "    progress_bar = setup_progress_bar() # í™˜ê²½ì— ë§ëŠ” tqdm ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "    # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™”\n",
    "    checkpoint_mgr = CheckpointManager(\"boardgame_pipeline_integrated_e5\")\n",
    "\n",
    "    # ë¦¬ì…‹ ì˜µì…˜ ì²˜ë¦¬\n",
    "    if reset_checkpoint:\n",
    "        print(\"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™” ìš”ì²­ë¨...\")\n",
    "        checkpoint_mgr.reset()\n",
    "        resume = False # ë¦¬ì…‹í•˜ë©´ ì¬ê°œí•  ìˆ˜ ì—†ìŒ\n",
    "\n",
    "    # ì¬ê°œ ëª¨ë“œ ì²˜ë¦¬\n",
    "    if resume:\n",
    "        resume_info = checkpoint_mgr.get_resume_info()\n",
    "        if resume_info[\"can_resume\"]:\n",
    "            print(\"\\nğŸ”„ ì´ì „ ì„¸ì…˜ì—ì„œ íŒŒì´í”„ë¼ì¸ ì¬ê°œ:\")\n",
    "            print(f\"   ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {resume_info['last_update']}\")\n",
    "            print(f\"   ì™„ë£Œëœ ë‹¨ê³„: {', '.join(resume_info['completed_stages'])}\")\n",
    "            # ì¹´ìš´í„° ì •ë³´ ì¶œë ¥ ë“±...\n",
    "        else:\n",
    "            print(\"âš ï¸ ì¬ê°œí•  ìˆ˜ ìˆëŠ” ì´ì „ ì„¸ì…˜ ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘.\")\n",
    "            resume = False # ì¬ê°œ ë¶ˆê°€ëŠ¥í•˜ë©´ ì²˜ìŒë¶€í„°\n",
    "\n",
    "    # ë³‘ë ¬ ì²˜ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”\n",
    "    parallel_mgr = ParallelProcessor(use_processes=parallel_use_processes) if use_parallel else None\n",
    "\n",
    "\n",
    "    # --- íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì‹¤í–‰ ---\n",
    "    try:\n",
    "        # 1. ë©”íƒ€ë°ì´í„° ë¡œë“œ\n",
    "        # ë©”íƒ€ë°ì´í„°ëŠ” ë³´í†µ ë³€ê²½ì´ ì¦ì§€ ì•Šìœ¼ë¯€ë¡œ, ì²´í¬í¬ì¸íŠ¸ ì™„ë£Œ ì‹œ ê±´ë„ˆë›°ì§€ ì•Šê³ \n",
    "        # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ íŒŒì¼ ê²½ë¡œ ë¹„êµ ë“±ì„ í†µí•´ ì¬ë¡œë“œë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŒ.\n",
    "        # ì—¬ê¸°ì„œëŠ” í•­ìƒ í˜¸ì¶œí•˜ë˜, ë‚´ë¶€ ë¡œì§ì´ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ë¥¼ í™œìš©í•˜ë„ë¡ í•¨.\n",
    "        metadata_dict, meta_id_column = load_game_metadata(\n",
    "            METADATA_CSV_PATH,\n",
    "            checkpoint_mgr,\n",
    "            progress_bar\n",
    "        )\n",
    "        if metadata_dict is None:\n",
    "             print(\"âŒ 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨.\")\n",
    "             return None\n",
    "\n",
    "        # 2. ë¦¬ë·° ë°ì´í„° ì²˜ë¦¬\n",
    "        review_df, review_column, review_id_column = preprocess_reviews(\n",
    "            REVIEW_CSV_PATH,\n",
    "            checkpoint_mgr,\n",
    "            progress_bar\n",
    "        )\n",
    "        # preprocess_reviews ë‚´ë¶€ì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ ì²˜ë¦¬ëœ ID ê±´ë„ˆëœ€\n",
    "        if review_df is None or review_column is None:\n",
    "             print(\"âŒ 2ë‹¨ê³„: ë¦¬ë·° ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨.\")\n",
    "             return None\n",
    "\n",
    "        # 3. ë°ì´í„° í†µí•© (Enrichment)\n",
    "        enriched_df = enrich_review_data_fixed(\n",
    "            review_df, metadata_dict, meta_id_column, review_id_column,\n",
    "            parallel_mgr, # ë³‘ë ¬ ê´€ë¦¬ì ì „ë‹¬\n",
    "            checkpoint_mgr,\n",
    "            progress_bar\n",
    "        )\n",
    "        if enriched_df is None or enriched_df.empty:\n",
    "             print(\"âŒ 3ë‹¨ê³„: ë°ì´í„° í†µí•© í›„ ìœ íš¨ ë°ì´í„° ì—†ìŒ. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨.\")\n",
    "             return None\n",
    "\n",
    "        # 4. Document ê°ì²´ ìƒì„±\n",
    "        documents = create_documents(\n",
    "            enriched_df, review_column, ENRICH_TEXT,\n",
    "            parallel_mgr, # ë³‘ë ¬ ê´€ë¦¬ì ì „ë‹¬\n",
    "            checkpoint_mgr,\n",
    "            progress_bar\n",
    "        )\n",
    "        if not documents:\n",
    "             print(\"âŒ 4ë‹¨ê³„: Document ê°ì²´ ìƒì„± ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨.\")\n",
    "             return None\n",
    "\n",
    "        # 5. ë¬¸ì„œ ë¶„í• \n",
    "        split_docs = split_documents(\n",
    "            documents, MAX_CHUNK_SIZE,\n",
    "            checkpoint_mgr,\n",
    "            progress_bar\n",
    "        )\n",
    "        # split_documents ë‚´ë¶€ì—ì„œ ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ë° ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "\n",
    "        # 6. ë²¡í„° ì €ì¥ì†Œ ì„¤ì •/ë¡œë“œ\n",
    "        vectorstore = setup_vectorstore(\n",
    "            MODEL_NAME, CHROMA_PERSIST_DIR, device,\n",
    "            checkpoint_mgr\n",
    "        )\n",
    "        if vectorstore is None:\n",
    "             print(\"âŒ 6ë‹¨ê³„: ë²¡í„° ì €ì¥ì†Œ ì„¤ì •/ë¡œë“œ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨.\")\n",
    "             return None\n",
    "         # --- <<< NEW: E5 ëª¨ë¸ Passage í¬ë§·íŒ… ì ìš© ë‹¨ê³„ >>> ---\n",
    "        formatted_docs_for_db = split_docs # ê¸°ë³¸ê°’ì€ ì›ë³¸ ì‚¬ìš©\n",
    "        formatting_stage_name = \"passage_formatting_applied\"\n",
    "        is_e5_model = MODEL_NAME.startswith(\"intfloat/e5\") or MODEL_NAME.startswith(\"intfloat/multilingual-e5\")\n",
    "\n",
    "        if is_e5_model:\n",
    "            print(\"\\n\" + \"=\"*70); print(\"ğŸ“œ 6.5ë‹¨ê³„: E5 Passage í¬ë§·íŒ… ì ìš©\"); print(\"=\"*70)\n",
    "            reload_formatting = True\n",
    "            if checkpoint_mgr and checkpoint_mgr.is_stage_completed(formatting_stage_name):\n",
    "                 print(\"âœ… Passage í¬ë§·íŒ…ì€ ì´ì „ ì™„ë£Œë¨ (ì²´í¬í¬ì¸íŠ¸)\")\n",
    "                 # reload_formatting = False # ê±´ë„ˆë›°ê¸° ì˜µì…˜\n",
    "\n",
    "            if reload_formatting:\n",
    "                st_format = time.time()\n",
    "                formatted_docs_list = []\n",
    "                for doc in progress_bar(split_docs, desc=\"Passage í¬ë§·íŒ…\"):\n",
    "                     formatted_content = format_passage(doc.page_content)\n",
    "                     # ë©”íƒ€ë°ì´í„° ìœ ì§€í•˜ë©° ìƒˆ Document ìƒì„±\n",
    "                     formatted_docs_list.append(Document(page_content=formatted_content, metadata=doc.metadata))\n",
    "                formatted_docs_for_db = formatted_docs_list\n",
    "                print(f\"âœ… í¬ë§·íŒ… ì™„ë£Œ: {len(formatted_docs_for_db):,}ê°œ ë¬¸ì„œ ({time.time()-st_format:.2f}ì´ˆ)\")\n",
    "                if checkpoint_mgr:\n",
    "                    checkpoint_mgr.update_stage(formatting_stage_name, True)\n",
    "                    print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {formatting_stage_name}=True\")\n",
    "            else:\n",
    "                 # í¬ë§·íŒ… ê±´ë„ˆë›¸ ê²½ìš°, ì´ì „ì— í¬ë§·íŒ…ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì•¼ í•¨.\n",
    "                 # ë¡œë“œ ë¡œì§ ì—†ìœ¼ë¯€ë¡œ ì¼ë‹¨ ì›ë³¸ ì‚¬ìš© (ê²½ê³  í‘œì‹œ)\n",
    "                 print(\"   âš ï¸ í¬ë§·íŒ… ê±´ë„ˆë›°ê¸° ì„ íƒë¨ (ë°ì´í„° ë¡œë“œ ë¡œì§ ë¶€ì¬ë¡œ ì›ë³¸ ì‚¬ìš©)\")\n",
    "                 formatted_docs_for_db = split_docs\n",
    "        else:\n",
    "             print(\"\\nâ„¹ï¸ E5 ëª¨ë¸ ì•„ë‹˜. Passage í¬ë§·íŒ… ê±´ë„ˆëœ€.\")\n",
    "        # --- <<< End of E5 Formatting Section >>> ---\n",
    "\n",
    "        # 7. ë²¡í„° ì €ì¥ì†Œì— ë¬¸ì„œ ì¶”ê°€\n",
    "        # add_documents_to_vectorstore ë‚´ë¶€ì—ì„œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¬ê°œ ë° ì¤‘ë³µ ë°©ì§€ ì²˜ë¦¬\n",
    "        vectorstore = add_documents_to_vectorstore(\n",
    "            split_docs, vectorstore, BATCH_SIZE,\n",
    "            checkpoint_mgr,\n",
    "            progress_bar\n",
    "        )\n",
    "        if vectorstore is None:\n",
    "             print(\"âŒ 7ë‹¨ê³„: ë¬¸ì„œ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨.\")\n",
    "             return None\n",
    "\n",
    "        # 8. ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸\n",
    "        vectorstore = test_vectorstore(vectorstore, checkpoint_mgr)\n",
    "\n",
    "\n",
    "        # --- íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ---\n",
    "        pipeline_time = time.time() - pipeline_start\n",
    "        minutes, seconds = divmod(pipeline_time, 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âœ… ë³´ë“œê²Œì„ ì¶”ì²œ ì‹œìŠ¤í…œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ê°œì„  ë²„ì „)\")\n",
    "        print(f\"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {int(hours)}ì‹œê°„ {int(minutes)}ë¶„ {seconds:.2f}ì´ˆ\")\n",
    "        print(f\"â±ï¸ ì™„ë£Œ ì‹œê°„: {time.strftime('%H:%M:%S')}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # ìµœì¢… ìš”ì•½ ì •ë³´ (ì²´í¬í¬ì¸íŠ¸ ì •ë³´ í™œìš©)\n",
    "        print(\"\\nğŸ“Š íŒŒì´í”„ë¼ì¸ ìš”ì•½ (ì²´í¬í¬ì¸íŠ¸ ê¸°ì¤€):\")\n",
    "        try:\n",
    "             final_state = checkpoint_mgr.get_resume_info()\n",
    "             counters = final_state[\"counters\"]\n",
    "             processed_counts = final_state[\"processed_counts\"]\n",
    "             db_count = vectorstore._collection.count() if vectorstore else 0\n",
    "\n",
    "             print(f\"   - ë¡œë“œëœ ë©”íƒ€ë°ì´í„° (ì¹´ìš´í„°): {counters.get('metadata_count', 0):,}\")\n",
    "             print(f\"   - ì²˜ë¦¬ëœ ë¦¬ë·° (ì¹´ìš´í„°): {counters.get('reviews_processed', 0):,}\")\n",
    "             print(f\"   - í†µí•©ëœ í–‰ (ì¹´ìš´í„°): {counters.get('enriched_count', 0):,}\")\n",
    "             print(f\"   - ìƒì„±ëœ Document (ì¹´ìš´í„°): {counters.get('documents_created', 0):,}\")\n",
    "             print(f\"   - ë¶„í• ëœ ë¬¸ì„œ (ì¹´ìš´í„°): {counters.get('documents_split', 0):,}\")\n",
    "             print(f\"   - DBì— ì¶”ê°€ëœ ë¬¸ì„œ ID (ëˆ„ì ): {processed_counts.get('documents', 0):,}\")\n",
    "             print(f\"   - ìµœì¢… ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œ ìˆ˜ (ì‹¤ì œ): {db_count:,}\")\n",
    "             print(f\"   - ìµœì¢… ë²¡í„° ì €ì¥ì†Œ ìœ„ì¹˜: {CHROMA_PERSIST_DIR}\")\n",
    "        except Exception as e_summary:\n",
    "             print(f\"   âš ï¸ ìš”ì•½ ì •ë³´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e_summary}\")\n",
    "\n",
    "        return vectorstore\n",
    "\n",
    "    # --- ì˜ˆì™¸ ì²˜ë¦¬ ---\n",
    "    except FileNotFoundError as fnf_error:\n",
    "         print(f\"\\nâŒ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: íŒŒì¼ ì—†ìŒ - {fnf_error}\")\n",
    "         return None\n",
    "    except ValueError as val_error:\n",
    "         print(f\"\\nâŒ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: ë°ì´í„° ë¬¸ì œ - {val_error}\")\n",
    "         return None\n",
    "    except ImportError as imp_error:\n",
    "         print(f\"\\nâŒ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - {imp_error}\")\n",
    "         return None\n",
    "    except KeyboardInterrupt:\n",
    "         print(\"\\nğŸš« ì‚¬ìš©ìì— ì˜í•´ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨ë¨.\")\n",
    "         # ì¤‘ë‹¨ ì‹œ í˜„ì¬ ìƒíƒœ ì €ì¥ ì‹œë„ (ì„ íƒì )\n",
    "         if checkpoint_mgr:\n",
    "              print(\"   í˜„ì¬ê¹Œì§€ì˜ ì§„í–‰ ìƒí™©ì„ ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "              checkpoint_mgr.save()\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í˜„ì¬ ìƒíƒœ ì €ì¥ ì‹œë„ (ì„ íƒì )\n",
    "        if checkpoint_mgr:\n",
    "             print(\"   ì˜¤ë¥˜ ë°œìƒ ì „ê¹Œì§€ì˜ ìƒíƒœë¥¼ ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥ ì‹œë„...\")\n",
    "             checkpoint_mgr.save()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âš ï¸ ë³´ë“œê²Œì„ ì¶”ì²œ ì‹œìŠ¤í…œ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨\")\n",
    "        print(f\"â±ï¸ ì¤‘ë‹¨ ì‹œê°„: {time.strftime('%H:%M:%S')}\")\n",
    "        print(\"=\"*70)\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\": # ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰ë  ë•Œë§Œ ì‹¤í–‰\n",
    "    print(\"--- íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ ---\")\n",
    "\n",
    "    # ì‹¤í–‰ ì˜µì…˜ ì„¤ì •\n",
    "    # ì˜ˆì‹œ: ì²˜ìŒ ì‹¤í–‰ ì‹œ\n",
    "    # final_vectorstore = run_pipeline(resume=False, reset_checkpoint=False, use_parallel=True)\n",
    "\n",
    "    # ì˜ˆì‹œ: ì¤‘ë‹¨ í›„ ì¬ê°œ ì‹œ\n",
    "    final_vectorstore = run_pipeline(resume=True, reset_checkpoint=False, use_parallel=True, parallel_use_processes=False)\n",
    "\n",
    "    # ì˜ˆì‹œ: ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹¤í–‰ (ì²´í¬í¬ì¸íŠ¸ ë¬´ì‹œ)\n",
    "    # final_vectorstore = run_pipeline(resume=False, reset_checkpoint=True, use_parallel=True)\n",
    "\n",
    "    if final_vectorstore:\n",
    "        print(\"\\n--- íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ. final_vectorstore ì‚¬ìš© ê°€ëŠ¥ ---\")\n",
    "        # ê°„ë‹¨í•œ ì¶”ê°€ í…ŒìŠ¤íŠ¸\n",
    "        # try:\n",
    "        #     test_query = \"í˜‘ë ¥ ê²Œì„\"\n",
    "        #     results = final_vectorstore.similarity_search(test_query, k=5)\n",
    "        #     print(f\"\\ní…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ('{test_query}'):\")`\n",
    "        #     for doc in results:\n",
    "        #         print(f\" - {doc.metadata.get('game_Title', 'ì œëª© ì—†ìŒ')}\")\n",
    "        # except Exception as e:\n",
    "        #      print(f\"ìµœì¢… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    else:\n",
    "        print(\"\\n--- íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ ë˜ëŠ” ì¤‘ë‹¨ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91a76e-a44a-4e96-b8e4-c50625531b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cd0e1-00db-40ee-bbac-84e0981f5339",
   "metadata": {},
   "source": [
    "## 5. ë©”íƒ€ë°ì´í„°ì™€ ë¦¬ë·° ë°ì´í„° í†µí•© (ìµœì í™” ë²„ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b0bbb-d779-4d89-9541-6f7fb3e34826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e5f58-83fa-4e03-b4cf-357501667110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481b086-0cee-4850-aa15-d59b82f13f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d66565-16f1-4417-983e-db65f7bc671b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3734aa-bddb-453a-8065-2cc0fbc53d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae586deb-429c-41b2-8a8b-6aa15f0c5a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}